{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda96521",
   "metadata": {},
   "source": [
    "### Text Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "087d6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"Data_Ingestion.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52d6fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16e72944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Data_Ingestion.txt'}, page_content='loader')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165a360",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc5b41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"Towards Real-Time Object.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c49eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f48b05ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='1\\nFaster R-CNN: Towards Real-Time Object\\nDetection with Region Proposal Networks\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\\nAbstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\\nAdvances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\\nproposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\\nconvolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\\nnetwork that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\\ngenerate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\\ninto a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\\n“attention” mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\\nour detection system has a frame rate of 5fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection\\naccuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\\n2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\\nmade publicly available.\\nIndex Terms—Object Detection, Region Proposal, Convolutional Neural Network.\\n!\\n1 I NTRODUCTION\\nRecent advances in object detection are driven by\\nthe success of region proposal methods ( e.g., [4])\\nand region-based convolutional neural networks (R-\\nCNNs) [5]. Although region-based CNNs were com-\\nputationally expensive as originally developed in [5],\\ntheir cost has been drastically reduced thanks to shar-\\ning convolutions across proposals [1], [2]. The latest\\nincarnation, Fast R-CNN [2], achieves near real-time\\nrates using very deep networks [3], when ignoring the\\ntime spent on region proposals . Now, proposals are the\\ntest-time computational bottleneck in state-of-the-art\\ndetection systems.\\nRegion proposal methods typically rely on inex-\\npensive features and economical inference schemes.\\nSelective Search [4], one of the most popular meth-\\nods, greedily merges superpixels based on engineered\\nlow-level features. Yet when compared to efﬁcient\\ndetection networks [2], Selective Search is an order of\\nmagnitude slower, at 2 seconds per image in a CPU\\nimplementation. EdgeBoxes [6] currently provides the\\nbest tradeoff between proposal quality and speed,\\nat 0.2 seconds per image. Nevertheless, the region\\nproposal step still consumes as much running time\\nas the detection network.\\n• S. Ren is with University of Science and Technology of China, Hefei,\\nChina. This work was done when S. Ren was an intern at Microsoft\\nResearch. Email: sqren@mail.ustc.edu.cn\\n• K. He and J. Sun are with Visual Computing Group, Microsoft\\nResearch. E-mail:{kahe,jiansun}@microsoft.com\\n• R. Girshick is with Facebook AI Research. The majority of this work\\nwas done when R. Girshick was with Microsoft Research. E-mail:\\nrbg@fb.com\\nOne may note that fast region-based CNNs take\\nadvantage of GPUs, while the region proposal meth-\\nods used in research are implemented on the CPU,\\nmaking such runtime comparisons inequitable. An ob-\\nvious way to accelerate proposal computation is to re-\\nimplement it for the GPU. This may be an effective en-\\ngineering solution, but re-implementation ignores the\\ndown-stream detection network and therefore misses\\nimportant opportunities for sharing computation.\\nIn this paper, we show that an algorithmic change—\\ncomputing proposals with a deep convolutional neu-\\nral network—leads to an elegant and effective solution\\nwhere proposal computation is nearly cost-free given\\nthe detection network’s computation. To this end, we\\nintroduce novel Region Proposal Networks (RPNs) that\\nshare convolutional layers with state-of-the-art object\\ndetection networks [1], [2]. By sharing convolutions at\\ntest-time, the marginal cost for computing proposals\\nis small ( e.g., 10ms per image).\\nOur observation is that the convolutional feature\\nmaps used by region-based detectors, like Fast R-\\nCNN, can also be used for generating region pro-\\nposals. On top of these convolutional features, we\\nconstruct an RPN by adding a few additional con-\\nvolutional layers that simultaneously regress region\\nbounds and objectness scores at each location on a\\nregular grid. The RPN is thus a kind of fully convo-\\nlutional network (FCN) [7] and can be trained end-to-\\nend speciﬁcally for the task for generating detection\\nproposals.\\nRPNs are designed to efﬁciently predict region pro-\\nposals with a wide range of scales and aspect ratios. In\\ncontrast to prevalent methods [8], [9], [1], [2] that use\\narXiv:1506.01497v3  [cs.CV]  6 Jan 2016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='2\\nmultiple scaled images\\nmultiple filter sizes\\nmultiple references\\n(a) (b) (c)\\nimage\\nfeature map\\nimage\\nfeature map\\nimage\\nfeature map\\nFigure 1: Different schemes for addressing multiple scales and sizes. (a) Pyramids of images and feature maps\\nare built, and the classiﬁer is run at all scales. (b) Pyramids of ﬁlters with multiple scales/sizes are run on\\nthe feature map. (c) We use pyramids of reference boxes in the regression functions.\\npyramids of images (Figure 1, a) or pyramids of ﬁlters\\n(Figure 1, b), we introduce novel “anchor” boxes\\nthat serve as references at multiple scales and aspect\\nratios. Our scheme can be thought of as a pyramid\\nof regression references (Figure 1, c), which avoids\\nenumerating images or ﬁlters of multiple scales or\\naspect ratios. This model performs well when trained\\nand tested using single-scale images and thus beneﬁts\\nrunning speed.\\nTo unify RPNs with Fast R-CNN [2] object detec-\\ntion networks, we propose a training scheme that\\nalternates between ﬁne-tuning for the region proposal\\ntask and then ﬁne-tuning for object detection, while\\nkeeping the proposals ﬁxed. This scheme converges\\nquickly and produces a uniﬁed network with convo-\\nlutional features that are shared between both tasks. 1\\nWe comprehensively evaluate our method on the\\nPASCAL VOC detection benchmarks [11] where RPNs\\nwith Fast R-CNNs produce detection accuracy bet-\\nter than the strong baseline of Selective Search with\\nFast R-CNNs. Meanwhile, our method waives nearly\\nall computational burdens of Selective Search at\\ntest-time—the effective running time for proposals\\nis just 10 milliseconds. Using the expensive very\\ndeep models of [3], our detection method still has\\na frame rate of 5fps ( including all steps ) on a GPU,\\nand thus is a practical object detection system in\\nterms of both speed and accuracy. We also report\\nresults on the MS COCO dataset [12] and investi-\\ngate the improvements on PASCAL VOC using the\\nCOCO data. Code has been made publicly available\\nat https://github.com/shaoqingren/faster_\\nrcnn (in MATLAB) and https://github.com/\\nrbgirshick/py-faster-rcnn (in Python).\\nA preliminary version of this manuscript was pub-\\nlished previously [10]. Since then, the frameworks of\\nRPN and Faster R-CNN have been adopted and gen-\\neralized to other methods, such as 3D object detection\\n[13], part-based detection [14], instance segmentation\\n[15], and image captioning [16]. Our fast and effective\\nobject detection system has also been built in com-\\n1. Since the publication of the conference version of this paper\\n[10], we have also found that RPNs can be trained jointly with Fast\\nR-CNN networks leading to less training time.\\nmercial systems such as at Pinterests [17], with user\\nengagement improvements reported.\\nIn ILSVRC and COCO 2015 competitions, Faster\\nR-CNN and RPN are the basis of several 1st-place\\nentries [18] in the tracks of ImageNet detection, Ima-\\ngeNet localization, COCO detection, and COCO seg-\\nmentation. RPNs completely learn to propose regions\\nfrom data, and thus can easily beneﬁt from deeper\\nand more expressive features (such as the 101-layer\\nresidual nets adopted in [18]). Faster R-CNN and RPN\\nare also used by several other leading entries in these\\ncompetitions2. These results suggest that our method\\nis not only a cost-efﬁcient solution for practical usage,\\nbut also an effective way of improving object detec-\\ntion accuracy.\\n2 R ELATED WORK\\nObject Proposals. There is a large literature on object\\nproposal methods. Comprehensive surveys and com-\\nparisons of object proposal methods can be found in\\n[19], [20], [21]. Widely used object proposal methods\\ninclude those based on grouping super-pixels ( e.g.,\\nSelective Search [4], CPMC [22], MCG [23]) and those\\nbased on sliding windows (e.g., objectness in windows\\n[24], EdgeBoxes [6]). Object proposal methods were\\nadopted as external modules independent of the de-\\ntectors ( e.g., Selective Search [4] object detectors, R-\\nCNN [5], and Fast R-CNN [2]).\\nDeep Networks for Object Detection. The R-CNN\\nmethod [5] trains CNNs end-to-end to classify the\\nproposal regions into object categories or background.\\nR-CNN mainly plays as a classiﬁer, and it does not\\npredict object bounds (except for reﬁning by bounding\\nbox regression). Its accuracy depends on the perfor-\\nmance of the region proposal module (see compar-\\nisons in [20]). Several papers have proposed ways of\\nusing deep networks for predicting object bounding\\nboxes [25], [9], [26], [27]. In the OverFeat method [9],\\na fully-connected layer is trained to predict the box\\ncoordinates for the localization task that assumes a\\nsingle object. The fully-connected layer is then turned\\n2. http://image-net.org/challenges/LSVRC/2015/results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='3\\nimage\\nconv layers\\nfeature maps\\nRegion Proposal Network\\nproposals\\nclassifier\\nRoI pooling\\nFigure 2: Faster R-CNN is a single, uniﬁed network\\nfor object detection. The RPN module serves as the\\n‘attention’ of this uniﬁed network.\\ninto a convolutional layer for detecting multiple class-\\nspeciﬁc objects. The MultiBox methods [26], [27] gen-\\nerate region proposals from a network whose last\\nfully-connected layer simultaneously predicts mul-\\ntiple class-agnostic boxes, generalizing the “single-\\nbox” fashion of OverFeat. These class-agnostic boxes\\nare used as proposals for R-CNN [5]. The MultiBox\\nproposal network is applied on a single image crop or\\nmultiple large image crops ( e.g., 224×224), in contrast\\nto our fully convolutional scheme. MultiBox does not\\nshare features between the proposal and detection\\nnetworks. We discuss OverFeat and MultiBox in more\\ndepth later in context with our method. Concurrent\\nwith our work, the DeepMask method [28] is devel-\\noped for learning segmentation proposals.\\nShared computation of convolutions [9], [1], [29],\\n[7], [2] has been attracting increasing attention for ef-\\nﬁcient, yet accurate, visual recognition. The OverFeat\\npaper [9] computes convolutional features from an\\nimage pyramid for classiﬁcation, localization, and de-\\ntection. Adaptively-sized pooling (SPP) [1] on shared\\nconvolutional feature maps is developed for efﬁcient\\nregion-based object detection [1], [30] and semantic\\nsegmentation [29]. Fast R-CNN [2] enables end-to-end\\ndetector training on shared convolutional features and\\nshows compelling accuracy and speed.\\n3 F ASTER R-CNN\\nOur object detection system, called Faster R-CNN, is\\ncomposed of two modules. The ﬁrst module is a deep\\nfully convolutional network that proposes regions,\\nand the second module is the Fast R-CNN detector [2]\\nthat uses the proposed regions. The entire system is a\\nsingle, uniﬁed network for object detection (Figure 2).\\nUsing the recently popular terminology of neural\\nnetworks with ‘attention’ [31] mechanisms, the RPN\\nmodule tells the Fast R-CNN module where to look.\\nIn Section 3.1 we introduce the designs and properties\\nof the network for region proposal. In Section 3.2 we\\ndevelop algorithms for training both modules with\\nfeatures shared.\\n3.1 Region Proposal Networks\\nA Region Proposal Network (RPN) takes an image\\n(of any size) as input and outputs a set of rectangular\\nobject proposals, each with an objectness score. 3 We\\nmodel this process with a fully convolutional network\\n[7], which we describe in this section. Because our ulti-\\nmate goal is to share computation with a Fast R-CNN\\nobject detection network [2], we assume that both nets\\nshare a common set of convolutional layers. In our ex-\\nperiments, we investigate the Zeiler and Fergus model\\n[32] (ZF), which has 5 shareable convolutional layers\\nand the Simonyan and Zisserman model [3] (VGG-16),\\nwhich has 13 shareable convolutional layers.\\nTo generate region proposals, we slide a small\\nnetwork over the convolutional feature map output\\nby the last shared convolutional layer. This small\\nnetwork takes as input an n×n spatial window of\\nthe input convolutional feature map. Each sliding\\nwindow is mapped to a lower-dimensional feature\\n(256-d for ZF and 512-d for VGG, with ReLU [33]\\nfollowing). This feature is fed into two sibling fully-\\nconnected layers—a box-regression layer ( reg) and a\\nbox-classiﬁcation layer ( cls). We use n = 3 in this\\npaper, noting that the effective receptive ﬁeld on the\\ninput image is large (171 and 228 pixels for ZF and\\nVGG, respectively). This mini-network is illustrated\\nat a single position in Figure 3 (left). Note that be-\\ncause the mini-network operates in a sliding-window\\nfashion, the fully-connected layers are shared across\\nall spatial locations. This architecture is naturally im-\\nplemented with an n×n convolutional layer followed\\nby two sibling 1× 1 convolutional layers (for reg and\\ncls, respectively).\\n3.1.1 Anchors\\nAt each sliding-window location, we simultaneously\\npredict multiple region proposals, where the number\\nof maximum possible proposals for each location is\\ndenoted ask. So the reg layer has 4k outputs encoding\\nthe coordinates of k boxes, and the cls layer outputs\\n2k scores that estimate probability of object or not\\nobject for each proposal 4. The k proposals are param-\\neterized relative to k reference boxes, which we call\\n3. “Region” is a generic term and in this paper we only consider\\nrectangular regions, as is common for many methods ( e.g., [27], [4],\\n[6]). “Objectness” measures membership to a set of object classes\\nvs. background.\\n4. For simplicity we implement the cls layer as a two-class\\nsoftmax layer. Alternatively, one may use logistic regression to\\nproduce k scores.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='4\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983 person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 3: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals on PASCAL\\nVOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nanchors. An anchor is centered at the sliding window\\nin question, and is associated with a scale and aspect\\nratio (Figure 3, left). By default we use 3 scales and\\n3 aspect ratios, yielding k = 9 anchors at each sliding\\nposition. For a convolutional feature map of a size\\nW×H (typically∼2,400), there are WHk anchors in\\ntotal.\\nT ranslation-Invariant Anchors\\nAn important property of our approach is that it\\nis translation invariant , both in terms of the anchors\\nand the functions that compute proposals relative to\\nthe anchors. If one translates an object in an image,\\nthe proposal should translate and the same function\\nshould be able to predict the proposal in either lo-\\ncation. This translation-invariant property is guaran-\\nteed by our method 5. As a comparison, the MultiBox\\nmethod [27] uses k-means to generate 800 anchors,\\nwhich are not translation invariant. So MultiBox does\\nnot guarantee that the same proposal is generated if\\nan object is translated.\\nThe translation-invariant property also reduces the\\nmodel size. MultiBox has a (4 + 1)× 800-dimensional\\nfully-connected output layer, whereas our method has\\na (4 + 2)× 9-dimensional convolutional output layer\\nin the case of k = 9 anchors. As a result, our output\\nlayer has 2.8× 104 parameters ( 512× (4 + 2) × 9\\nfor VGG-16), two orders of magnitude fewer than\\nMultiBox’s output layer that has 6.1× 106 parameters\\n(1536× (4 + 1)× 800 for GoogleNet [34] in MultiBox\\n[27]). If considering the feature projection layers, our\\nproposal layers still have an order of magnitude fewer\\nparameters than MultiBox 6. We expect our method\\nto have less risk of overﬁtting on small datasets, like\\nPASCAL VOC.\\n5. As is the case of FCNs [7], our network is translation invariant\\nup to the network’s total stride.\\n6. Considering the feature projection layers, our proposal layers’\\nparameter count is 3× 3× 512× 512 + 512× 6× 9 = 2 .4× 106;\\nMultiBox’s proposal layers’ parameter count is 7× 7× (64 + 96 +\\n64 + 64)× 1536 + 1536× 5× 800 = 27× 106.\\nMulti-Scale Anchors as Regression References\\nOur design of anchors presents a novel scheme\\nfor addressing multiple scales (and aspect ratios). As\\nshown in Figure 1, there have been two popular ways\\nfor multi-scale predictions. The ﬁrst way is based on\\nimage/feature pyramids, e.g., in DPM [8] and CNN-\\nbased methods [9], [1], [2]. The images are resized at\\nmultiple scales, and feature maps (HOG [8] or deep\\nconvolutional features [9], [1], [2]) are computed for\\neach scale (Figure 1(a)). This way is often useful but\\nis time-consuming. The second way is to use sliding\\nwindows of multiple scales (and/or aspect ratios) on\\nthe feature maps. For example, in DPM [8], models\\nof different aspect ratios are trained separately using\\ndifferent ﬁlter sizes (such as 5×7 and 7×5). If this way\\nis used to address multiple scales, it can be thought\\nof as a “pyramid of ﬁlters” (Figure 1(b)). The second\\nway is usually adopted jointly with the ﬁrst way [8].\\nAs a comparison, our anchor-based method is built\\non a pyramid of anchors , which is more cost-efﬁcient.\\nOur method classiﬁes and regresses bounding boxes\\nwith reference to anchor boxes of multiple scales and\\naspect ratios. It only relies on images and feature\\nmaps of a single scale, and uses ﬁlters (sliding win-\\ndows on the feature map) of a single size. We show by\\nexperiments the effects of this scheme for addressing\\nmultiple scales and sizes (Table 8).\\nBecause of this multi-scale design based on anchors,\\nwe can simply use the convolutional features com-\\nputed on a single-scale image, as is also done by\\nthe Fast R-CNN detector [2]. The design of multi-\\nscale anchors is a key component for sharing features\\nwithout extra cost for addressing scales.\\n3.1.2 Loss Function\\nFor training RPNs, we assign a binary class label\\n(of being an object or not) to each anchor. We as-\\nsign a positive label to two kinds of anchors: (i) the\\nanchor/anchors with the highest Intersection-over-\\nUnion (IoU) overlap with a ground-truth box, or (ii) an\\nanchor that has an IoU overlap higher than 0.7 with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='5\\nany ground-truth box. Note that a single ground-truth\\nbox may assign positive labels to multiple anchors.\\nUsually the second condition is sufﬁcient to determine\\nthe positive samples; but we still adopt the ﬁrst\\ncondition for the reason that in some rare cases the\\nsecond condition may ﬁnd no positive sample. We\\nassign a negative label to a non-positive anchor if its\\nIoU ratio is lower than 0.3 for all ground-truth boxes.\\nAnchors that are neither positive nor negative do not\\ncontribute to the training objective.\\nWith these deﬁnitions, we minimize an objective\\nfunction following the multi-task loss in Fast R-CNN\\n[2]. Our loss function for an image is deﬁned as:\\nL({pi},{ti}) = 1\\nNcls\\n∑\\ni\\nLcls (pi,p∗\\ni )\\n+λ 1\\nNreg\\n∑\\ni\\np∗\\niLreg(ti,t∗\\ni ).\\n(1)\\nHere, i is the index of an anchor in a mini-batch and\\npi is the predicted probability of anchor i being an\\nobject. The ground-truth label p∗\\ni is 1 if the anchor\\nis positive, and is 0 if the anchor is negative. ti is a\\nvector representing the 4 parameterized coordinates\\nof the predicted bounding box, and t∗\\ni is that of the\\nground-truth box associated with a positive anchor.\\nThe classiﬁcation lossLcls is log loss over two classes\\n(object vs. not object). For the regression loss, we use\\nLreg(ti,t∗\\ni ) = R(ti− t∗\\ni ) where R is the robust loss\\nfunction (smooth L 1) deﬁned in [2]. The term p∗\\niLreg\\nmeans the regression loss is activated only for positive\\nanchors (p∗\\ni = 1 ) and is disabled otherwise ( p∗\\ni = 0 ).\\nThe outputs of the cls and reg layers consist of {pi}\\nand{ti} respectively.\\nThe two terms are normalized by Ncls and Nreg\\nand weighted by a balancing parameter λ. In our\\ncurrent implementation (as in the released code), the\\ncls term in Eqn.(1) is normalized by the mini-batch\\nsize ( i.e., Ncls = 256 ) and the reg term is normalized\\nby the number of anchor locations ( i.e.,Nreg∼ 2, 400).\\nBy default we set λ = 10 , and thus both cls and\\nreg terms are roughly equally weighted. We show\\nby experiments that the results are insensitive to the\\nvalues of λ in a wide range (Table 9). We also note\\nthat the normalization as above is not required and\\ncould be simpliﬁed.\\nFor bounding box regression, we adopt the param-\\neterizations of the 4 coordinates following [5]:\\ntx = (x−xa)/wa, t y = (y−ya)/ha,\\ntw = log(w/wa), t h = log(h/ha),\\nt∗\\nx = (x∗−xa)/wa, t ∗\\ny = (y∗−ya)/ha,\\nt∗\\nw = log(w∗/wa), t ∗\\nh = log(h∗/ha),\\n(2)\\nwhere x, y, w, and h denote the box’s center coordi-\\nnates and its width and height. Variables x, xa, and\\nx∗ are for the predicted box, anchor box, and ground-\\ntruth box respectively (likewise for y,w,h ). This can\\nbe thought of as bounding-box regression from an\\nanchor box to a nearby ground-truth box.\\nNevertheless, our method achieves bounding-box\\nregression by a different manner from previous RoI-\\nbased (Region of Interest) methods [1], [2]. In [1],\\n[2], bounding-box regression is performed on features\\npooled from arbitrarily sized RoIs, and the regression\\nweights are shared by all region sizes. In our formula-\\ntion, the features used for regression are of the same\\nspatial size ( 3× 3) on the feature maps. To account\\nfor varying sizes, a set of k bounding-box regressors\\nare learned. Each regressor is responsible for one scale\\nand one aspect ratio, and the k regressors do not share\\nweights. As such, it is still possible to predict boxes of\\nvarious sizes even though the features are of a ﬁxed\\nsize/scale, thanks to the design of anchors.\\n3.1.3 T raining RPNs\\nThe RPN can be trained end-to-end by back-\\npropagation and stochastic gradient descent (SGD)\\n[35]. We follow the “image-centric” sampling strategy\\nfrom [2] to train this network. Each mini-batch arises\\nfrom a single image that contains many positive and\\nnegative example anchors. It is possible to optimize\\nfor the loss functions of all anchors, but this will\\nbias towards negative samples as they are dominate.\\nInstead, we randomly sample 256 anchors in an image\\nto compute the loss function of a mini-batch, where\\nthe sampled positive and negative anchors have a\\nratio of up to 1:1. If there are fewer than 128 positive\\nsamples in an image, we pad the mini-batch with\\nnegative ones.\\nWe randomly initialize all new layers by drawing\\nweights from a zero-mean Gaussian distribution with\\nstandard deviation 0.01. All other layers ( i.e., the\\nshared convolutional layers) are initialized by pre-\\ntraining a model for ImageNet classiﬁcation [36], as\\nis standard practice [5]. We tune all layers of the\\nZF net, and conv3 1 and up for the VGG net to\\nconserve memory [2]. We use a learning rate of 0.001\\nfor 60k mini-batches, and 0.0001 for the next 20k\\nmini-batches on the PASCAL VOC dataset. We use a\\nmomentum of 0.9 and a weight decay of 0.0005 [37].\\nOur implementation uses Caffe [38].\\n3.2 Sharing Features for RPN and Fast R-CNN\\nThus far we have described how to train a network\\nfor region proposal generation, without considering\\nthe region-based object detection CNN that will utilize\\nthese proposals. For the detection network, we adopt\\nFast R-CNN [2]. Next we describe algorithms that\\nlearn a uniﬁed network composed of RPN and Fast\\nR-CNN with shared convolutional layers (Figure 2).\\nBoth RPN and Fast R-CNN, trained independently,\\nwill modify their convolutional layers in different\\nways. We therefore need to develop a technique that\\nallows for sharing convolutional layers between the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='6\\nTable 1: the learned average proposal size for each anchor using the ZF net (numbers for s = 600).\\nanchor 1282, 2:1 1282, 1:1 1282, 1:2 2562, 2:1 2562, 1:1 2562, 1:2 5122, 2:1 5122, 1:1 5122, 1:2\\nproposal 188×111 113×114 70×92 416×229 261×284 174×332 768×437 499×501 355×715\\ntwo networks, rather than learning two separate net-\\nworks. We discuss three ways for training networks\\nwith features shared:\\n(i) Alternating training. In this solution, we ﬁrst train\\nRPN, and use the proposals to train Fast R-CNN.\\nThe network tuned by Fast R-CNN is then used to\\ninitialize RPN, and this process is iterated. This is the\\nsolution that is used in all experiments in this paper.\\n(ii) Approximate joint training . In this solution, the\\nRPN and Fast R-CNN networks are merged into one\\nnetwork during training as in Figure 2. In each SGD\\niteration, the forward pass generates region propos-\\nals which are treated just like ﬁxed, pre-computed\\nproposals when training a Fast R-CNN detector. The\\nbackward propagation takes place as usual, where for\\nthe shared layers the backward propagated signals\\nfrom both the RPN loss and the Fast R-CNN loss\\nare combined. This solution is easy to implement. But\\nthis solution ignores the derivative w.r.t. the proposal\\nboxes’ coordinates that are also network responses,\\nso is approximate. In our experiments, we have em-\\npirically found this solver produces close results, yet\\nreduces the training time by about 25-50% comparing\\nwith alternating training. This solver is included in\\nour released Python code.\\n(iii) Non-approximate joint training . As discussed\\nabove, the bounding boxes predicted by RPN are\\nalso functions of the input. The RoI pooling layer\\n[2] in Fast R-CNN accepts the convolutional features\\nand also the predicted bounding boxes as input, so\\na theoretically valid backpropagation solver should\\nalso involve gradients w.r.t. the box coordinates. These\\ngradients are ignored in the above approximate joint\\ntraining. In a non-approximate joint training solution,\\nwe need an RoI pooling layer that is differentiable\\nw.r.t. the box coordinates. This is a nontrivial problem\\nand a solution can be given by an “RoI warping” layer\\nas developed in [15], which is beyond the scope of this\\npaper.\\n4-Step Alternating T raining. In this paper, we adopt\\na pragmatic 4-step training algorithm to learn shared\\nfeatures via alternating optimization. In the ﬁrst step,\\nwe train the RPN as described in Section 3.1.3. This\\nnetwork is initialized with an ImageNet-pre-trained\\nmodel and ﬁne-tuned end-to-end for the region pro-\\nposal task. In the second step, we train a separate\\ndetection network by Fast R-CNN using the proposals\\ngenerated by the step-1 RPN. This detection net-\\nwork is also initialized by the ImageNet-pre-trained\\nmodel. At this point the two networks do not share\\nconvolutional layers. In the third step, we use the\\ndetector network to initialize RPN training, but we\\nﬁx the shared convolutional layers and only ﬁne-tune\\nthe layers unique to RPN. Now the two networks\\nshare convolutional layers. Finally, keeping the shared\\nconvolutional layers ﬁxed, we ﬁne-tune the unique\\nlayers of Fast R-CNN. As such, both networks share\\nthe same convolutional layers and form a uniﬁed\\nnetwork. A similar alternating training can be run\\nfor more iterations, but we have observed negligible\\nimprovements.\\n3.3 Implementation Details\\nWe train and test both region proposal and object\\ndetection networks on images of a single scale [1], [2].\\nWe re-scale the images such that their shorter side\\nis s = 600 pixels [2]. Multi-scale feature extraction\\n(using an image pyramid) may improve accuracy but\\ndoes not exhibit a good speed-accuracy trade-off [2].\\nOn the re-scaled images, the total stride for both ZF\\nand VGG nets on the last convolutional layer is 16\\npixels, and thus is ∼10 pixels on a typical PASCAL\\nimage before resizing (∼500×375). Even such a large\\nstride provides good results, though accuracy may be\\nfurther improved with a smaller stride.\\nFor anchors, we use 3 scales with box areas of 1282,\\n2562, and 5122 pixels, and 3 aspect ratios of 1:1, 1:2,\\nand 2:1. These hyper-parameters are not carefully cho-\\nsen for a particular dataset, and we provide ablation\\nexperiments on their effects in the next section. As dis-\\ncussed, our solution does not need an image pyramid\\nor ﬁlter pyramid to predict regions of multiple scales,\\nsaving considerable running time. Figure 3 (right)\\nshows the capability of our method for a wide range\\nof scales and aspect ratios. Table 1 shows the learned\\naverage proposal size for each anchor using the ZF\\nnet. We note that our algorithm allows predictions\\nthat are larger than the underlying receptive ﬁeld.\\nSuch predictions are not impossible—one may still\\nroughly infer the extent of an object if only the middle\\nof the object is visible.\\nThe anchor boxes that cross image boundaries need\\nto be handled with care. During training, we ignore\\nall cross-boundary anchors so they do not contribute\\nto the loss. For a typical 1000× 600 image, there\\nwill be roughly 20000 ( ≈ 60× 40× 9) anchors in\\ntotal. With the cross-boundary anchors ignored, there\\nare about 6000 anchors per image for training. If the\\nboundary-crossing outliers are not ignored in training,\\nthey introduce large, difﬁcult to correct error terms in\\nthe objective, and training does not converge. During\\ntesting, however, we still apply the fully convolutional\\nRPN to the entire image. This may generate cross-\\nboundary proposal boxes, which we clip to the image\\nboundary.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='7\\nTable 2: Detection results on PASCAL VOC 2007 test set (trained on VOC 2007 trainval). The detectors are\\nFast R-CNN with ZF, but using various proposal methods for training and testing.\\ntrain-time region proposals test-time region proposals\\nmethod # boxes method # proposals mAP (%)\\nSS 2000 SS 2000 58.7\\nEB 2000 EB 2000 58.6\\nRPN+ZF, shared 2000 RPN+ZF, shared 300 59.9\\nablation experiments follow below\\nRPN+ZF, unshared 2000 RPN+ZF, unshared 300 58.7\\nSS 2000 RPN+ZF 100 55.1\\nSS 2000 RPN+ZF 300 56.8\\nSS 2000 RPN+ZF 1000 56.3\\nSS 2000 RPN+ZF (no NMS) 6000 55.2\\nSS 2000 RPN+ZF (no cls) 100 44.6\\nSS 2000 RPN+ZF (no cls) 300 51.4\\nSS 2000 RPN+ZF (no cls) 1000 55.8\\nSS 2000 RPN+ZF (no reg) 300 52.1\\nSS 2000 RPN+ZF (no reg) 1000 51.3\\nSS 2000 RPN+VGG 300 59.2\\nSome RPN proposals highly overlap with each\\nother. To reduce redundancy, we adopt non-maximum\\nsuppression (NMS) on the proposal regions based on\\ntheir cls scores. We ﬁx the IoU threshold for NMS\\nat 0.7, which leaves us about 2000 proposal regions\\nper image. As we will show, NMS does not harm the\\nultimate detection accuracy, but substantially reduces\\nthe number of proposals. After NMS, we use the\\ntop-N ranked proposal regions for detection. In the\\nfollowing, we train Fast R-CNN using 2000 RPN pro-\\nposals, but evaluate different numbers of proposals at\\ntest-time.\\n4 E XPERIMENTS\\n4.1 Experiments on P ASCAL VOC\\nWe comprehensively evaluate our method on the\\nPASCAL VOC 2007 detection benchmark [11]. This\\ndataset consists of about 5k trainval images and 5k\\ntest images over 20 object categories. We also provide\\nresults on the PASCAL VOC 2012 benchmark for a\\nfew models. For the ImageNet pre-trained network,\\nwe use the “fast” version of ZF net [32] that has\\n5 convolutional layers and 3 fully-connected layers,\\nand the public VGG-16 model 7 [3] that has 13 con-\\nvolutional layers and 3 fully-connected layers. We\\nprimarily evaluate detection mean Average Precision\\n(mAP), because this is the actual metric for object\\ndetection (rather than focusing on object proposal\\nproxy metrics).\\nTable 2 (top) shows Fast R-CNN results when\\ntrained and tested using various region proposal\\nmethods. These results use the ZF net. For Selective\\nSearch (SS) [4], we generate about 2000 proposals by\\nthe “fast” mode. For EdgeBoxes (EB) [6], we generate\\nthe proposals by the default EB setting tuned for 0.7\\n7. www.robots.ox.ac.uk/∼vgg/research/very deep/\\nIoU. SS has an mAP of 58.7% and EB has an mAP\\nof 58.6% under the Fast R-CNN framework. RPN\\nwith Fast R-CNN achieves competitive results, with\\nan mAP of 59.9% while using up to 300 proposals 8.\\nUsing RPN yields a much faster detection system than\\nusing either SS or EB because of shared convolutional\\ncomputations; the fewer proposals also reduce the\\nregion-wise fully-connected layers’ cost (Table 5).\\nAblation Experiments on RPN. To investigate the be-\\nhavior of RPNs as a proposal method, we conducted\\nseveral ablation studies. First, we show the effect of\\nsharing convolutional layers between the RPN and\\nFast R-CNN detection network. To do this, we stop\\nafter the second step in the 4-step training process.\\nUsing separate networks reduces the result slightly to\\n58.7% (RPN+ZF, unshared, Table 2). We observe that\\nthis is because in the third step when the detector-\\ntuned features are used to ﬁne-tune the RPN, the\\nproposal quality is improved.\\nNext, we disentangle the RPN’s inﬂuence on train-\\ning the Fast R-CNN detection network. For this pur-\\npose, we train a Fast R-CNN model by using the\\n2000 SS proposals and ZF net. We ﬁx this detector\\nand evaluate the detection mAP by changing the\\nproposal regions used at test-time. In these ablation\\nexperiments, the RPN does not share features with\\nthe detector.\\nReplacing SS with 300 RPN proposals at test-time\\nleads to an mAP of 56.8%. The loss in mAP is because\\nof the inconsistency between the training/testing pro-\\nposals. This result serves as the baseline for the fol-\\nlowing comparisons.\\nSomewhat surprisingly, the RPN still leads to a\\ncompetitive result (55.1%) when using the top-ranked\\n8. For RPN, the number of proposals ( e.g., 300) is the maximum\\nnumber for an image. RPN may produce fewer proposals after\\nNMS, and thus the average number of proposals is smaller.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='8\\nTable 3: Detection results on PASCAL VOC 2007 test set . The detector is Fast R-CNN and VGG-16. Training\\ndata: “07”: VOC 2007 trainval, “07+12”: union set of VOC 2007 trainval and VOC 2012 trainval. For RPN,\\nthe train-time proposals for Fast R-CNN are 2000. †: this number was reported in [2]; using the repository\\nprovided by this paper, this result is higher (68.1).\\nmethod # proposals data mAP (%)\\nSS 2000 07 66.9†\\nSS 2000 07+12 70.0\\nRPN+VGG, unshared 300 07 68.5\\nRPN+VGG, shared 300 07 69.9\\nRPN+VGG, shared 300 07+12 73.2\\nRPN+VGG, shared 300 COCO+07+12 78.8\\nTable 4: Detection results on PASCAL VOC 2012 test set . The detector is Fast R-CNN and VGG-16. Training\\ndata: “07”: VOC 2007 trainval, “07++12”: union set of VOC 2007 trainval+test and VOC 2012 trainval. For\\nRPN, the train-time proposals for Fast R-CNN are 2000. †: http://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html.‡:\\nhttp://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html.§: http://host.robots.ox.ac.uk:8080/anonymous/XEDH10.html.\\nmethod # proposals data mAP (%)\\nSS 2000 12 65.7\\nSS 2000 07++12 68.4\\nRPN+VGG, shared† 300 12 67.0\\nRPN+VGG, shared‡ 300 07++12 70.4\\nRPN+VGG, shared§ 300 COCO+07++12 75.9\\nTable 5: Timing (ms) on a K40 GPU, except SS proposal is evaluated in a CPU. “Region-wise” includes NMS,\\npooling, fully-connected, and softmax layers. See our released code for the proﬁling of running time.\\nmodel system conv proposal region-wise total rate\\nVGG SS + Fast R-CNN 146 1510 174 1830 0.5 fps\\nVGG RPN + Fast R-CNN 141 10 47 198 5 fps\\nZF RPN + Fast R-CNN 31 3 25 59 17 fps\\n100 proposals at test-time, indicating that the top-\\nranked RPN proposals are accurate. On the other\\nextreme, using the top-ranked 6000 RPN proposals\\n(without NMS) has a comparable mAP (55.2%), sug-\\ngesting NMS does not harm the detection mAP and\\nmay reduce false alarms.\\nNext, we separately investigate the roles of RPN’s\\ncls and reg outputs by turning off either of them\\nat test-time. When the cls layer is removed at test-\\ntime (thus no NMS/ranking is used), we randomly\\nsample N proposals from the unscored regions. The\\nmAP is nearly unchanged with N = 1000 (55.8%), but\\ndegrades considerably to 44.6% when N = 100 . This\\nshows that the cls scores account for the accuracy of\\nthe highest ranked proposals.\\nOn the other hand, when the reg layer is removed\\nat test-time (so the proposals become anchor boxes),\\nthe mAP drops to 52.1%. This suggests that the high-\\nquality proposals are mainly due to the regressed box\\nbounds. The anchor boxes, though having multiple\\nscales and aspect ratios, are not sufﬁcient for accurate\\ndetection.\\nWe also evaluate the effects of more powerful net-\\nworks on the proposal quality of RPN alone. We use\\nVGG-16 to train the RPN, and still use the above\\ndetector of SS+ZF. The mAP improves from 56.8%\\n(using RPN+ZF) to 59.2% (using RPN+VGG). This is a\\npromising result, because it suggests that the proposal\\nquality of RPN+VGG is better than that of RPN+ZF.\\nBecause proposals of RPN+ZF are competitive with\\nSS (both are 58.7% when consistently used for training\\nand testing), we may expect RPN+VGG to be better\\nthan SS. The following experiments justify this hy-\\npothesis.\\nPerformance of VGG-16. Table 3 shows the results\\nof VGG-16 for both proposal and detection. Using\\nRPN+VGG, the result is 68.5% for unshared features,\\nslightly higher than the SS baseline. As shown above,\\nthis is because the proposals generated by RPN+VGG\\nare more accurate than SS. Unlike SS that is pre-\\ndeﬁned, the RPN is actively trained and beneﬁts from\\nbetter networks. For the feature- shared variant, the\\nresult is 69.9%—better than the strong SS baseline, yet\\nwith nearly cost-free proposals. We further train the\\nRPN and detection network on the union set of PAS-\\nCAL VOC 2007 trainval and 2012 trainval. The mAP\\nis 73.2%. Figure 5 shows some results on the PASCAL\\nVOC 2007 test set. On the PASCAL VOC 2012 test set\\n(Table 4), our method has an mAP of 70.4% trained\\non the union set of VOC 2007 trainval+test and VOC\\n2012 trainval. Table 6 and Table 7 show the detailed\\nnumbers.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='9\\nTable 6: Results on PASCAL VOC 2007 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time\\nproposals for Fast R-CNN are 2000. RPN ∗ denotes the unsharing feature version.\\nmethod # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\\nSS 2000 07 66.9 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8\\nSS 2000 07+12 70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4\\nRPN∗ 300 07 68.5 74.1 77.2 67.7 53.9 51.0 75.1 79.2 78.9 50.7 78.0 61.1 79.1 81.9 72.2 75.9 37.2 71.4 62.5 77.4 66.4\\nRPN 300 07 69.9 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6\\nRPN 300 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\\nRPN 300 COCO+07+1278.8 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9\\nTable 7: Results on PASCAL VOC 2012 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time\\nproposals for Fast R-CNN are 2000.\\nmethod # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\\nSS 2000 12 65.7 80.3 74.7 66.9 46.9 37.7 73.9 68.6 87.7 41.7 71.1 51.1 86.0 77.8 79.8 69.8 32.1 65.5 63.8 76.4 61.7\\nSS 2000 07++12 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2\\nRPN 300 12 67.0 82.3 76.4 71.0 48.4 45.2 72.1 72.3 87.3 42.2 73.7 50.0 86.8 78.7 78.4 77.4 34.5 70.1 57.1 77.1 58.9\\nRPN 300 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\\nRPN 300 COCO+07++1275.9 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2\\nTable 8: Detection results of Faster R-CNN on PAS-\\nCAL VOC 2007 test set using different settings of\\nanchors. The network is VGG-16. The training data\\nis VOC 2007 trainval. The default setting of using 3\\nscales and 3 aspect ratios (69.9%) is the same as that\\nin Table 3.\\nsettings anchor scales aspect ratios mAP (%)\\n1 scale, 1 ratio 1282 1:1 65.8\\n2562 1:1 66.7\\n1 scale, 3 ratios 1282 {2:1, 1:1, 1:2} 68.8\\n2562 {2:1, 1:1, 1:2} 67.9\\n3 scales, 1 ratio {1282, 2562, 5122} 1:1 69.8\\n3 scales, 3 ratios {1282, 2562, 5122}{2:1, 1:1, 1:2} 69.9\\nTable 9: Detection results of Faster R-CNN on PAS-\\nCAL VOC 2007 test set using different values of λ\\nin Equation (1). The network is VGG-16. The training\\ndata is VOC 2007 trainval. The default setting of using\\nλ = 10 (69.9%) is the same as that in Table 3.\\nλ 0.1 1 10 100\\nmAP (%) 67.2 68.9 69.9 69.1\\nIn Table 5 we summarize the running time of the\\nentire object detection system. SS takes 1-2 seconds\\ndepending on content (on average about 1.5s), and\\nFast R-CNN with VGG-16 takes 320ms on 2000 SS\\nproposals (or 223ms if using SVD on fully-connected\\nlayers [2]). Our system with VGG-16 takes in total\\n198ms for both proposal and detection. With the con-\\nvolutional features shared, the RPN alone only takes\\n10ms computing the additional layers. Our region-\\nwise computation is also lower, thanks to fewer pro-\\nposals (300 per image). Our system has a frame-rate\\nof 17 fps with the ZF net.\\nSensitivities to Hyper-parameters. In Table 8 we\\ninvestigate the settings of anchors. By default we use\\n3 scales and 3 aspect ratios (69.9% mAP in Table 8).\\nIf using just one anchor at each position, the mAP\\ndrops by a considerable margin of 3-4%. The mAP\\nis higher if using 3 scales (with 1 aspect ratio) or 3\\naspect ratios (with 1 scale), demonstrating that using\\nanchors of multiple sizes as the regression references\\nis an effective solution. Using just 3 scales with 1\\naspect ratio (69.8%) is as good as using 3 scales with\\n3 aspect ratios on this dataset, suggesting that scales\\nand aspect ratios are not disentangled dimensions for\\nthe detection accuracy. But we still adopt these two\\ndimensions in our designs to keep our system ﬂexible.\\nIn Table 9 we compare different values ofλ in Equa-\\ntion (1). By default we use λ = 10 which makes the\\ntwo terms in Equation (1) roughly equally weighted\\nafter normalization. Table 9 shows that our result is\\nimpacted just marginally (by ∼ 1%) when λ is within\\na scale of about two orders of magnitude (1 to 100).\\nThis demonstrates that the result is insensitive to λ in\\na wide range.\\nAnalysis of Recall-to-IoU. Next we compute the\\nrecall of proposals at different IoU ratios with ground-\\ntruth boxes. It is noteworthy that the Recall-to-IoU\\nmetric is just loosely [19], [20], [21] related to the\\nultimate detection accuracy. It is more appropriate to\\nuse this metric to diagnose the proposal method than\\nto evaluate it.\\nIn Figure 4, we show the results of using 300, 1000,\\nand 2000 proposals. We compare with SS and EB, and\\nthe N proposals are the top-N ranked ones based on\\nthe conﬁdence generated by these methods. The plots\\nshow that the RPN method behaves gracefully when\\nthe number of proposals drops from 2000 to 300. This\\nexplains why the RPN has a good ultimate detection\\nmAP when using as few as 300 proposals. As we\\nanalyzed before, this property is mainly attributed to\\nthe cls term of the RPN. The recall of SS and EB drops\\nmore quickly than RPN when the proposals are fewer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='10\\n/g1004/g856/g1009/g1004/g856/g1010/g1004/g856/g1011/g1004/g856/g1012/g1004/g856/g1013/g1005/g1004\\n/g1004/g856/g1006\\n/g1004/g856/g1008\\n/g1004/g856/g1010\\n/g1004/g856/g1012\\n/g1005\\n/g47/g381/g104\\n/g90/g286/g272/g258/g367/g367\\n/g1007/g1004/g1004/g3/g393/g396/g381/g393/g381/g400/g258/g367/g400\\n \\n \\n/g94/g94\\n/g28/g17\\n/g90/g87/g69/g3/g127/g38\\n/g90/g87/g69/g3/g115/g39/g39\\n/g1004/g856/g1009/g1004/g856/g1010/g1004/g856/g1011/g1004/g856/g1012/g1004/g856/g1013/g1005/g1004\\n/g1004/g856/g1006\\n/g1004/g856/g1008\\n/g1004/g856/g1010\\n/g1004/g856/g1012\\n/g1005\\n/g47/g381/g104\\n/g1005/g1004/g1004/g1004/g3/g393/g396/g381/g393/g381/g400/g258/g367/g400\\n \\n \\n/g94/g94\\n/g28/g17\\n/g90/g87/g69/g3/g127/g38\\n/g90/g87/g69/g3/g115/g39/g39\\n/g1004/g856/g1009/g1004/g856/g1010/g1004/g856/g1011/g1004/g856/g1012/g1004/g856/g1013/g1005/g1004\\n/g1004/g856/g1006\\n/g1004/g856/g1008\\n/g1004/g856/g1010\\n/g1004/g856/g1012\\n/g1005\\n/g47/g381/g104\\n/g1006/g1004/g1004/g1004/g3/g393/g396/g381/g393/g381/g400/g258/g367/g400\\n \\n \\n/g94/g94\\n/g28/g17\\n/g90/g87/g69/g3/g127/g38\\n/g90/g87/g69/g3/g115/g39/g39\\nFigure 4: Recall vs. IoU overlap ratio on the PASCAL VOC 2007 test set.\\nTable 10: One-Stage Detection vs. Two-Stage Proposal + Detection . Detection results are on the PASCAL\\nVOC 2007 test set using the ZF model and Fast R-CNN. RPN uses unshared features.\\nproposals detector mAP (%)\\nTwo-Stage RPN + ZF, unshared 300 Fast R-CNN + ZF, 1 scale 58.7\\nOne-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 1 scale 53.8\\nOne-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 5 scales 53.9\\nOne-Stage Detection vs. Two-Stage Proposal + De-\\ntection. The OverFeat paper [9] proposes a detection\\nmethod that uses regressors and classiﬁers on sliding\\nwindows over convolutional feature maps. OverFeat\\nis a one-stage, class-speciﬁc detection pipeline, and ours\\nis a two-stage cascade consisting of class-agnostic pro-\\nposals and class-speciﬁc detections. In OverFeat, the\\nregion-wise features come from a sliding window of\\none aspect ratio over a scale pyramid. These features\\nare used to simultaneously determine the location and\\ncategory of objects. In RPN, the features are from\\nsquare (3×3) sliding windows and predict proposals\\nrelative to anchors with different scales and aspect\\nratios. Though both methods use sliding windows, the\\nregion proposal task is only the ﬁrst stage of Faster R-\\nCNN—the downstream Fast R-CNN detector attends\\nto the proposals to reﬁne them. In the second stage of\\nour cascade, the region-wise features are adaptively\\npooled [1], [2] from proposal boxes that more faith-\\nfully cover the features of the regions. We believe\\nthese features lead to more accurate detections.\\nTo compare the one-stage and two-stage systems,\\nwe emulate the OverFeat system (and thus also circum-\\nvent other differences of implementation details) by\\none-stage Fast R-CNN. In this system, the “proposals”\\nare dense sliding windows of 3 scales (128, 256, 512)\\nand 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is\\ntrained to predict class-speciﬁc scores and regress box\\nlocations from these sliding windows. Because the\\nOverFeat system adopts an image pyramid, we also\\nevaluate using convolutional features extracted from\\n5 scales. We use those 5 scales as in [1], [2].\\nTable 10 compares the two-stage system and two\\nvariants of the one-stage system. Using the ZF model,\\nthe one-stage system has an mAP of 53.9%. This is\\nlower than the two-stage system (58.7%) by 4.8%.\\nThis experiment justiﬁes the effectiveness of cascaded\\nregion proposals and object detection. Similar obser-\\nvations are reported in [2], [39], where replacing SS\\nregion proposals with sliding windows leads to ∼6%\\ndegradation in both papers. We also note that the one-\\nstage system is slower as it has considerably more\\nproposals to process.\\n4.2 Experiments on MS COCO\\nWe present more results on the Microsoft COCO\\nobject detection dataset [12]. This dataset involves 80\\nobject categories. We experiment with the 80k images\\non the training set, 40k images on the validation set,\\nand 20k images on the test-dev set. We evaluate the\\nmAP averaged for IoU ∈ [0.5 : 0 .05 : 0 .95] (COCO’s\\nstandard metric, simply denoted as mAP@[.5, .95])\\nand mAP@0.5 (PASCAL VOC’s metric).\\nThere are a few minor changes of our system made\\nfor this dataset. We train our models on an 8-GPU\\nimplementation, and the effective mini-batch size be-\\ncomes 8 for RPN (1 per GPU) and 16 for Fast R-CNN\\n(2 per GPU). The RPN step and Fast R-CNN step are\\nboth trained for 240k iterations with a learning rate\\nof 0.003 and then for 80k iterations with 0.0003. We\\nmodify the learning rates (starting with 0.003 instead\\nof 0.001) because the mini-batch size is changed. For\\nthe anchors, we use 3 aspect ratios and 4 scales\\n(adding 642), mainly motivated by handling small\\nobjects on this dataset. In addition, in our Fast R-CNN\\nstep, the negative samples are deﬁned as those with\\na maximum IoU with ground truth in the interval of\\n[0, 0.5), instead of [0.1, 0.5) used in [1], [2]. We note\\nthat in the SPPnet system [1], the negative samples\\nin [0.1, 0.5) are used for network ﬁne-tuning, but the\\nnegative samples in [0, 0.5) are still visited in the SVM\\nstep with hard-negative mining. But the Fast R-CNN\\nsystem [2] abandons the SVM step, so the negative\\nsamples in [0, 0.1) are never visited. Including these\\n[0, 0.1) samples improves mAP@0.5 on the COCO\\ndataset for both Fast R-CNN and Faster R-CNN sys-\\ntems (but the impact is negligible on PASCAL VOC).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='11\\nTable 11: Object detection results (%) on the MS COCO dataset. The model is VGG-16.\\nCOCO val COCO test-dev\\nmethod proposals training data mAP@.5 mAP@[.5, .95] mAP@.5 mAP@[.5, .95]\\nFast R-CNN [2] SS, 2000 COCO train - - 35.9 19.7\\nFast R-CNN [impl. in this paper] SS, 2000 COCO train 38.6 18.9 39.3 19.3\\nFaster R-CNN RPN, 300 COCO train 41.5 21.2 42.1 21.5\\nFaster R-CNN RPN, 300 COCO trainval - - 42.7 21.9\\nThe rest of the implementation details are the same\\nas on PASCAL VOC. In particular, we keep using\\n300 proposals and single-scale ( s = 600 ) testing. The\\ntesting time is still about 200ms per image on the\\nCOCO dataset.\\nIn Table 11 we ﬁrst report the results of the Fast\\nR-CNN system [2] using the implementation in this\\npaper. Our Fast R-CNN baseline has 39.3% mAP@0.5\\non the test-dev set, higher than that reported in [2].\\nWe conjecture that the reason for this gap is mainly\\ndue to the deﬁnition of the negative samples and also\\nthe changes of the mini-batch sizes. We also note that\\nthe mAP@[.5, .95] is just comparable.\\nNext we evaluate our Faster R-CNN system. Using\\nthe COCO training set to train, Faster R-CNN has\\n42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the\\nCOCO test-dev set. This is 2.8% higher for mAP@0.5\\nand 2.2% higher for mAP@[.5, .95] than the Fast R-\\nCNN counterpart under the same protocol (Table 11).\\nThis indicates that RPN performs excellent for im-\\nproving the localization accuracy at higher IoU thresh-\\nolds. Using the COCO trainval set to train, Faster R-\\nCNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on\\nthe COCO test-dev set. Figure 6 shows some results\\non the MS COCO test-dev set.\\nFaster R-CNN in ILSVRC & COCO 2015 compe-\\ntitions We have demonstrated that Faster R-CNN\\nbeneﬁts more from better features, thanks to the fact\\nthat the RPN completely learns to propose regions by\\nneural networks. This observation is still valid even\\nwhen one increases the depth substantially to over\\n100 layers [18]. Only by replacing VGG-16 with a 101-\\nlayer residual net (ResNet-101) [18], the Faster R-CNN\\nsystem increases the mAP from 41.5%/21.2% (VGG-\\n16) to 48.4%/27.2% (ResNet-101) on the COCO val\\nset. With other improvements orthogonal to Faster R-\\nCNN, He et al. [18] obtained a single-model result of\\n55.7%/34.9% and an ensemble result of 59.0%/37.4%\\non the COCO test-dev set, which won the 1st place\\nin the COCO 2015 object detection competition. The\\nsame system [18] also won the 1st place in the ILSVRC\\n2015 object detection competition, surpassing the sec-\\nond place by absolute 8.5%. RPN is also a building\\nblock of the 1st-place winning entries in ILSVRC 2015\\nlocalization and COCO 2015 segmentation competi-\\ntions, for which the details are available in [18] and\\n[15] respectively.\\nTable 12: Detection mAP (%) of Faster R-CNN on\\nPASCAL VOC 2007 test set and 2012 test set us-\\ning different training data. The model is VGG-16.\\n“COCO” denotes that the COCO trainval set is used\\nfor training. See also Table 6 and Table 7.\\ntraining data 2007 test 2012 test\\nVOC07 69.9 67.0\\nVOC07+12 73.2 -\\nVOC07++12 - 70.4\\nCOCO (no VOC) 76.1 73.0\\nCOCO+VOC07+12 78.8 -\\nCOCO+VOC07++12 - 75.9\\n4.3 From MS COCO to P ASCAL VOC\\nLarge-scale data is of crucial importance for improv-\\ning deep neural networks. Next, we investigate how\\nthe MS COCO dataset can help with the detection\\nperformance on PASCAL VOC.\\nAs a simple baseline, we directly evaluate the\\nCOCO detection model on the PASCAL VOC dataset,\\nwithout ﬁne-tuning on any P ASCAL VOC data . This\\nevaluation is possible because the categories on\\nCOCO are a superset of those on PASCAL VOC. The\\ncategories that are exclusive on COCO are ignored in\\nthis experiment, and the softmax layer is performed\\nonly on the 20 categories plus background. The mAP\\nunder this setting is 76.1% on the PASCAL VOC 2007\\ntest set (Table 12). This result is better than that trained\\non VOC07+12 (73.2%) by a good margin, even though\\nthe PASCAL VOC data are not exploited.\\nThen we ﬁne-tune the COCO detection model on\\nthe VOC dataset. In this experiment, the COCO model\\nis in place of the ImageNet-pre-trained model (that\\nis used to initialize the network weights), and the\\nFaster R-CNN system is ﬁne-tuned as described in\\nSection 3.2. Doing so leads to 78.8% mAP on the\\nPASCAL VOC 2007 test set. The extra data from\\nthe COCO set increases the mAP by 5.6%. Table 6\\nshows that the model trained on COCO+VOC has\\nthe best AP for every individual category on PASCAL\\nVOC 2007. Similar improvements are observed on the\\nPASCAL VOC 2012 test set (Table 12 and Table 7). We\\nnote that the test-time speed of obtaining these strong\\nresults is still about 200ms per image .\\n5 C ONCLUSION\\nWe have presented RPNs for efﬁcient and accurate\\nregion proposal generation. By sharing convolutional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='12\\nbottle : 0.726\\nperson : 0.992\\ndog : 0.981\\nbicycle : 0.987\\nbicycle : 0.977\\n7\\n7\\n7\\nbicycle : 0.972\\nperson : 0.995\\nperson : 0.994\\n e : 0.987e:09 8 7\\nbicycbicyc\\nbicycb\\n4\\n44person : 0.981\\nperson : 0.975\\nperson : 0.972\\nperson : 0.948\\nperson : 0.919\\nhorse : 0.984\\nperson : 0.670\\nbird : 0.997\\nbird : 0.727\\ncar : 1.000\\ncar : 0.982\\ncar car car : 0.981\\n car : 0.880\\nbottle : 0.826\\nchair : 0.630\\ndiningtable : 0.862\\npottedplant : 0.728\\nboat : 0.995\\nboat : 0.948\\nboat : 0.808\\n: 0 808:boat : 0.692\\nboat : 0.992\\nboat : 0.846\\nboat : 0.693\\nbottle : 0.962\\n0 962\\nbottle : 0.851\\ndiningtable : 0.791\\nperson : 0.962\\nperson : 0.930\\npottedplant : 0.951\\ndog : 0.987\\nperson : 0.940\\n940940person : 0.893\\ncat : 0.998\\ncar : 1.000\\nperson : 0.917\\nboat : 0.895\\nboat : 0.877\\nboat : 0.749\\nboat : 0.671\\nperson : 0.988\\ncar : 0.955\\n55\\n55car : 0.745\\n.745\\nhorse : 0.991\\nperson : 0.988\\nperson : 0.797\\n bird : 0.978\\nbird : 0.972\\nbird : 0.941\\nbird : 0.902\\nperson : 0.918\\ncow : 0.998\\ncow : 0.995\\naeroplane : 0.992\\naeroplane : 0.986\\n sheep : 0.970\\nbird : 0.998\\nbird : 0.980\\nbird : 0.806\\npottedplant : 0.993\\npottedplant : 0.940\\npottedplant : 0.869\\npottedplant : 0.820\\npottedplant : 0.715\\naeroplane : 0.998\\ncar : 0.907\\n907907person : 0.993\\n person : 0.987\\nchair : 0.984\\nchair : 0.978\\nchair : 0.976\\nchair : 0.962\\n984984diningtable : 0.997\\nbottle : 0.789\\nchair : 0.723\\ndiningtable : 0.903\\ne : 0.789\\nperson : 0.968\\ntvmonitor : 0.993\\ntvmonitor : 0.945\\naeroplane : 0.978\\nperson : 0.988\\nbottle : 0.903\\nbottle : 0.884\\nbottle : 0.858\\nbb\\nbottle : 0bot\\nbottle : 0.616\\nchair : 0.982\\nchair : 0.852\\nperson : 0.983\\nperson : 0.959\\n: 0 903person : 0.897\\nperson : 0.870\\ntvmonitor : 0.993\\ndog : 0.697\\nperson : 0.961\\nperson : 0.960\\npersonperson\\nperson : 0.958\\nperson : 0.757\\nbus : 0.999\\nperson : 0.996\\nper\\nperperperson : 0.995\\nperson : 0.994\\nperson : 0.985\\ncow : 0.985\\ncow : 0.979\\ncow : 0.979\\ncow : 0.974\\ncow : 0.892\\nperson : 0.998\\ncar : 0.999\\nperson : 0.929\\n person : 0.994\\nperson : 0.991\\nperson : 0.988\\npersp\\nperson : 0.976\\nperson : 0.964\\ncar : 0.997\\n car : 0.980\\nperson : 0.993\\npersonperson\\npersonpersonperson : 0.986\\n 0 993\\n:n\\n86\\nn:\\nn:nperson : 0.959\\nFigure 5: Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster\\nR-CNN system. The model is VGG-16 and the training data is 07+12 trainval (73.2% mAP on the 2007 test\\nset). Our method detects objects of a wide range of scales and aspect ratios. Each output box is associated\\nwith a category label and a softmax score in [0, 1]. A score threshold of 0.6 is used to display these images.\\nThe running time for obtaining these results is 198ms per image, including all steps .\\nfeatures with the down-stream detection network, the\\nregion proposal step is nearly cost-free. Our method\\nenables a uniﬁed, deep-learning-based object detec-\\ntion system to run at near real-time frame rates. The\\nlearned RPN also improves region proposal quality\\nand thus the overall object detection accuracy.\\nREFERENCES\\n[1] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling\\nin deep convolutional networks for visual recognition,” in\\nEuropean Conference on Computer Vision (ECCV) , 2014.\\n[2] R. Girshick, “Fast R-CNN,” in IEEE International Conference on\\nComputer Vision (ICCV) , 2015.\\n[3] K. Simonyan and A. Zisserman, “Very deep convolutional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='13\\ncup : 0.807\\nbowl : 0.847\\nbowl : 0.816\\nbowl : 0.744\\nbowl : 0.710\\nchair : 0.772\\ndining table : 0.618\\noven : 0.969\\nrefrigerator : 0.631\\ncup : 0.990\\npizza : 0.919\\ndining table : 0.888\\n person : 0.984\\npersonpersoncar : 0.816\\npizza : 0.965\\nclock : 0.988\\nperson : 0.998\\nkite : 0.934\\ntoothbrush : 0.668\\nteddy bear : 0.999\\nteddy bear : 0.890\\nteddy bear : 0.802\\nteddy bear : 0.738\\nbowl : 0.602\\npotted plant : 0.769\\ntoilet : 0.921\\nsink : 0.969\\nsink : 0.994\\nsink : 0.992\\nsink : 0.976\\n66sink : 0.938\\nperson : 0.970\\n: 0.970\\nersonperson : 0.869\\nbus : 0.999\\nbottle : 0.768\\ncup : 0.720\\nchair : 0.644\\ntv : 0.964\\ntv : 0.959\\nlaptop : 0.986\\nmouse : 0.871\\nmouse : 0.677\\nm\\nkeyboard : 0.956\\nbook : 0.611\\nperson : 0.986\\nboat : 0.758\\n boat : 0.746\\nboat : 0.613\\nbench : 0.971\\ntrain : 0.965\\ntraffic light : 0.869\\n traffic light : 0.713\\nchair : 0.631\\ncouch : 0.991\\ncouch : 0.719\\ncouch : 0.627\\ndining table : 0.637\\ndog : 0.966\\nfrisbee : 0.998\\nbird : 0.987\\nbird : 0.968\\nbird : 0.894\\nperson : 0.723\\ncup : 0.986\\ncup : 0.931\\nbowl : 0.958\\nsandwich : 0.629\\ndining table : 0.941\\nzebra : 0.996\\nzebra : 0.993\\nzebra : 0.970\\n970zebra : 0.848\\nperson : 0.917\\nperson : 0.792\\n: 0.7920 792\\ntv : 0.711\\nlaptop : 0.973\\nmouse : 0.981\\nkeyboard : 0.638\\nkeyboard : 0.615\\nperson : 0.999\\nperson : 0.999person : 0.999\\npersopersotennis racket : 0.960\\nbird : 0.956\\nbird : 0.906\\nbird : 0.746\\nhorse : 0.990\\nperson : 0.993\\nbottle : 0.982\\noven : 0.655\\nrefrigerator : 0.699\\nclock : 0.982\\nbed : 0.999\\nperson : 0.808\\nbottle : 0.627\\npizza : 0.995\\npizza : 0.985\\npizza : 0.982\\npizza : 0.938\\ndining table : 0.956\\nperson : 0.998\\nskis : 0.919\\nbowl : 0.759\\nbroccoli : 0.953\\nperson : 0.999\\nperson : 0.934\\nsurfboard : 0.979\\nperson : 0.940\\n person : 0.927\\nperson : 0.864\\n0.940\\nperson : 0.854\\nperson : 0.825\\n5\\n5person : 0.813\\nperson : 0.716\\nperson : 0.692\\np\\npperson : 0.691\\n927\\n927person : 0.665\\nperson : 0.618\\nboat : 0.992\\numbrella : 0.885\\ngiraffe : 0.993\\ngiraffe : 0.989\\ngiraffe : 0.988\\nperson : 0.867\\nairplane : 0.997\\nperson : 0.970\\nperson : 0.950\\n person : 0.931\\np\\nperson : 0.916\\nperson : 0.897\\nperson : 0.842\\n person : 0.841\\nperson : 0.84person : 0.772\\nbicycle : 0.891\\nbicycle : 0.639\\ncar : 0.957\\nmotorcycle : 0.827\\nmotorcycle : 0.713\\ntraffic light : 0.802\\numbrella : 0.824\\nperson : 0.800\\nclock : 0.986\\nclock : 0.981\\nperson : 0.996\\nperson : 0.976\\nperson : 0.975\\nrson : 0.975rson\\nson : 0onperson : 0.958\\nperson : 0.950\\nperson : 0.941\\n0.976\\n0 976person : 0.939\\npepe\\nperson : 0.928\\n 958\\n95\\n8\\n09 75\\nn : 0.\\nn : 0\\n.\\n0.975\\n0.9750.person : 0.823\\non : 0.9500 50\\nperson : 0.805\\nperson : 0.766\\nperson : 0.759\\n.9414\\nperson : 0.673\\ndog : 0.996\\ndog : 0.691\\n0 939\\np\\nbackpack : 0.756\\nhandbag : 0.848\\nFigure 6: Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN\\nsystem. The model is VGG-16 and the training data is COCO trainval (42.7% mAP@0.5 on the test-dev set).\\nEach output box is associated with a category label and a softmax score in [0, 1]. A score threshold of 0.6 is\\nused to display these images. For each image, one color represents one object category in that image.\\nnetworks for large-scale image recognition,” in International\\nConference on Learning Representations (ICLR) , 2015.\\n[4] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeul-\\nders, “Selective search for object recognition,” International\\nJournal of Computer Vision (IJCV) , 2013.\\n[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\\nhierarchies for accurate object detection and semantic seg-\\nmentation,” in IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2014.\\n[6] C. L. Zitnick and P . Doll ´ar, “Edge boxes: Locating object\\nproposals from edges,” in European Conference on Computer\\nVision (ECCV), 2014.\\n[7] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional\\nnetworks for semantic segmentation,” in IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2015.\\n[8] P . F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\\nmanan, “Object detection with discriminatively trained part-\\nbased models,” IEEE Transactions on Pattern Analysis and Ma-\\nchine Intelligence (TP AMI), 2010.\\n[9] P . Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y. LeCun, “Overfeat: Integrated recognition, localization\\nand detection using convolutional networks,” in International\\nConference on Learning Representations (ICLR) , 2014.\\n[10] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-01-07T01:06:15+00:00', 'author': '', 'keywords': '', 'moddate': '2016-01-07T01:06:15+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Towards Real-Time Object.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='14\\nreal-time object detection with region proposal networks,” in\\nNeural Information Processing Systems (NIPS) , 2015.\\n[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman, “The PASCAL Visual Object Classes Challenge\\n2007 (VOC2007) Results,” 2007.\\n[12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ra-\\nmanan, P . Doll´ar, and C. L. Zitnick, “Microsoft COCO: Com-\\nmon Objects in Context,” in European Conference on Computer\\nVision (ECCV), 2014.\\n[13] S. Song and J. Xiao, “Deep sliding shapes for amodal 3d object\\ndetection in rgb-d images,” arXiv:1511.02300, 2015.\\n[14] J. Zhu, X. Chen, and A. L. Yuille, “DeePM: A deep part-based\\nmodel for object detection and semantic part localization,”\\narXiv:1511.07131, 2015.\\n[15] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmenta-\\ntion via multi-task network cascades,” arXiv:1512.04412, 2015.\\n[16] J. Johnson, A. Karpathy, and L. Fei-Fei, “Densecap: Fully\\nconvolutional localization networks for dense captioning,”\\narXiv:1511.07571, 2015.\\n[17] D. Kislyuk, Y. Liu, D. Liu, E. Tzeng, and Y. Jing, “Human cu-\\nration and convnets: Powering item-to-item recommendations\\non pinterest,” arXiv:1511.04003, 2015.\\n[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\\nfor image recognition,” arXiv:1512.03385, 2015.\\n[19] J. Hosang, R. Benenson, and B. Schiele, “How good are de-\\ntection proposals, really?” in British Machine Vision Conference\\n(BMVC), 2014.\\n[20] J. Hosang, R. Benenson, P . Doll ´ar, and B. Schiele, “What makes\\nfor effective detection proposals?” IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence (TP AMI), 2015.\\n[21] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra,\\n“Object-Proposal Evaluation Protocol is ’Gameable’,” arXiv:\\n1505.05836, 2015.\\n[22] J. Carreira and C. Sminchisescu, “CPMC: Automatic ob-\\nject segmentation using constrained parametric min-cuts,”\\nIEEE Transactions on Pattern Analysis and Machine Intelligence\\n(TP AMI), 2012.\\n[23] P . Arbel´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n“Multiscale combinatorial grouping,” in IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2014.\\n[24] B. Alexe, T. Deselaers, and V . Ferrari, “Measuring the object-\\nness of image windows,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence (TP AMI), 2012.\\n[25] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks\\nfor object detection,” in Neural Information Processing Systems\\n(NIPS), 2013.\\n[26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable\\nobject detection using deep neural networks,” in IEEE Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) , 2014.\\n[27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, “Scalable,\\nhigh-quality object detection,” arXiv:1412.1441 (v1), 2015.\\n[28] P . O. Pinheiro, R. Collobert, and P . Dollar, “Learning to\\nsegment object candidates,” in Neural Information Processing\\nSystems (NIPS), 2015.\\n[29] J. Dai, K. He, and J. Sun, “Convolutional feature masking\\nfor joint object and stuff segmentation,” in IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2015.\\n[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Ob-\\nject detection networks on convolutional feature maps,”\\narXiv:1504.06066, 2015.\\n[31] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and\\nY. Bengio, “Attention-based models for speech recognition,”\\nin Neural Information Processing Systems (NIPS) , 2015.\\n[32] M. D. Zeiler and R. Fergus, “Visualizing and understanding\\nconvolutional neural networks,” in European Conference on\\nComputer Vision (ECCV) , 2014.\\n[33] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve\\nrestricted boltzmann machines,” in International Conference on\\nMachine Learning (ICML) , 2010.\\n[34] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, and A. Rabinovich, “Going deeper with convo-\\nlutions,” in IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2015.\\n[35] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\\nW. Hubbard, and L. D. Jackel, “Backpropagation applied to\\nhandwritten zip code recognition,” Neural computation, 1989.\\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,\\nand L. Fei-Fei, “ImageNet Large Scale Visual Recognition\\nChallenge,” in International Journal of Computer Vision (IJCV) ,\\n2015.\\n[37] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classi-\\nﬁcation with deep convolutional neural networks,” in Neural\\nInformation Processing Systems (NIPS) , 2012.\\n[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\\nshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional\\narchitecture for fast feature embedding,”arXiv:1408.5093, 2014.\\n[39] K. Lenc and A. Vedaldi, “R-CNN minus R,” in British Machine\\nVision Conference (BMVC), 2015.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "181cd70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514c400",
   "metadata": {},
   "source": [
    "### Web Based Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b45eb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(web_paths=('https://arxiv.org/abs/2304.10557',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "457db2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b5438b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/abs/2304.10557', 'title': '[2304.10557] An Introduction to Transformers', 'description': 'Abstract page for arXiv paper 2304.10557: An Introduction to Transformers', 'language': 'en'}, page_content=\"\\n\\n [2304.10557] An Introduction to Transformers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\\nDonate\\n\\n\\n\\n\\n\\n > cs > arXiv:2304.10557\\n  \\n\\n\\n\\n\\n\\n\\n\\nHelp | Advanced Search\\n\\n\\n\\n\\nAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull text\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nopen search\\n\\n\\n\\n\\n\\n\\nGO\\n\\n\\n\\nopen navigation menu\\n\\n\\nquick links\\n\\nLogin\\nHelp Pages\\nAbout\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComputer Science > Machine Learning\\n\\n\\narXiv:2304.10557 (cs)\\n    \\n\\n\\n\\n\\n  [Submitted on 20 Apr 2023 (v1), last revised 20 Jan 2026 (this version, v6)]\\nTitle:An Introduction to Transformers\\nAuthors:Richard E. Turner View a PDF of the paper titled An Introduction to Transformers, by Richard E. Turner\\nView PDF\\n\\nAbstract:The transformer is a neural network component that can be used to learn useful representations of sequences or sets of data-points. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture. We will not discuss training as this is rather standard. We assume that the reader is familiar with fundamental topics in machine learning including multi-layer perceptrons, linear transformations, softmax functions and basic probability.\\n    \\n\\n\\n\\nSubjects:\\n\\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)\\n\\nCite as:\\narXiv:2304.10557 [cs.LG]\\n\\n\\n\\xa0\\n(or \\narXiv:2304.10557v6 [cs.LG] for this version)\\n          \\n\\n\\n\\xa0\\n https://doi.org/10.48550/arXiv.2304.10557\\n\\n\\nFocus to learn more\\n\\n\\n\\n                  arXiv-issued DOI via DataCite\\n\\n\\n\\n\\n\\n\\n\\nSubmission history From: Richard Turner [view email]       [v1]\\n        Thu, 20 Apr 2023 14:54:19 UTC (297 KB)\\n[v2]\\n        Mon, 24 Apr 2023 12:56:07 UTC (327 KB)\\n[v3]\\n        Tue, 4 Jul 2023 11:05:08 UTC (297 KB)\\n[v4]\\n        Thu, 19 Oct 2023 15:52:39 UTC (371 KB)\\n[v5]\\n        Thu, 8 Feb 2024 15:01:34 UTC (372 KB)\\n[v6]\\n        Tue, 20 Jan 2026 09:43:51 UTC (404 KB)\\n\\n\\n\\n \\n\\nFull-text links:\\nAccess Paper:\\n\\n\\nView a PDF of the paper titled An Introduction to Transformers, by Richard E. TurnerView PDFTeX Source\\n \\n\\n\\nview license\\n\\n\\n \\n    Current browse context: cs.LG\\n\\n\\n<\\xa0prev\\n\\n\\xa0 | \\xa0 \\nnext\\xa0>\\n\\n\\nnew\\n | \\nrecent\\n | 2023-04\\n\\n    Change to browse by:\\n    \\ncs\\ncs.AI\\n\\n\\n\\n\\nReferences & Citations\\n\\nNASA ADSGoogle Scholar\\nSemantic Scholar\\n\\n\\n\\n\\n\\n 1 blog link (what is this?)\\n        \\n\\n\\nexport BibTeX citation\\nLoading...\\n\\n\\n\\n\\nBibTeX formatted citation\\n×\\n\\n\\nloading...\\n\\n\\nData provided by: \\n\\n\\n\\n\\nBookmark\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nBibliographic Tools\\n\\nBibliographic and Citation Tools\\n\\n\\n\\n\\n\\n\\nBibliographic Explorer Toggle\\n\\n\\n\\nBibliographic Explorer (What is the Explorer?)\\n\\n\\n\\n\\n\\n\\n\\nConnected Papers Toggle\\n\\n\\n\\nConnected Papers (What is Connected Papers?)\\n\\n\\n\\n\\n\\n\\nLitmaps Toggle\\n\\n\\n\\nLitmaps (What is Litmaps?)\\n\\n\\n\\n\\n\\n\\n\\nscite.ai Toggle\\n\\n\\n\\nscite Smart Citations (What are Smart Citations?)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode, Data, Media\\n\\nCode, Data and Media Associated with this Article\\n\\n\\n\\n\\n\\n\\nalphaXiv Toggle\\n\\n\\n\\nalphaXiv (What is alphaXiv?)\\n\\n\\n\\n\\n\\n\\n\\nLinks to Code Toggle\\n\\n\\n\\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\\n\\n\\n\\n\\n\\n\\n\\nDagsHub Toggle\\n\\n\\n\\nDagsHub (What is DagsHub?)\\n\\n\\n\\n\\n\\n\\n\\nGotitPub Toggle\\n\\n\\n\\nGotit.pub (What is GotitPub?)\\n\\n\\n\\n\\n\\n\\n\\nHuggingface Toggle\\n\\n\\n\\nHugging Face (What is Huggingface?)\\n\\n\\n\\n\\n\\n\\n\\nLinks to Code Toggle\\n\\n\\n\\nPapers with Code (What is Papers with Code?)\\n\\n\\n\\n\\n\\n\\n\\nScienceCast Toggle\\n\\n\\n\\nScienceCast (What is ScienceCast?)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDemos\\n\\nDemos\\n\\n\\n\\n\\n\\n\\nReplicate Toggle\\n\\n\\n\\nReplicate (What is Replicate?)\\n\\n\\n\\n\\n\\n\\n\\nSpaces Toggle\\n\\n\\n\\nHugging Face Spaces (What is Spaces?)\\n\\n\\n\\n\\n\\n\\n\\nSpaces Toggle\\n\\n\\n\\nTXYZ.AI (What is TXYZ.AI?)\\n\\n\\n\\n\\n\\n\\n\\n\\nRelated Papers\\n\\nRecommenders and Search Tools\\n\\n\\n\\n\\n\\n\\nLink to Influence Flower\\n\\n\\n\\nInfluence Flower (What are Influence Flowers?)\\n\\n\\n\\n\\n\\n\\n\\nCore recommender toggle\\n\\n\\n\\nCORE Recommender (What is CORE?)\\n\\n \\n\\n\\n\\n\\nIArxiv recommender toggle\\n\\n\\n\\nIArxiv Recommender\\n(What is IArxiv?)\\n\\n\\n\\n\\n\\n\\nAuthor\\nVenue\\nInstitution\\nTopic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        About arXivLabs\\n      \\n\\n\\n\\narXivLabs: experimental projects with community collaborators\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhich authors of this paper are endorsers? |\\n    Disable MathJax (What is MathJax?)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\nHelp\\n\\n\\n\\n\\n\\ncontact arXivClick here to contact arXiv\\n Contact\\n\\n\\nsubscribe to arXiv mailingsClick here to subscribe\\n Subscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright\\nPrivacy Policy\\n\\n\\n\\n\\nWeb Accessibility Assistance\\n\\n\\narXiv Operational Status \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159725c6",
   "metadata": {},
   "source": [
    "### Arxiv loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3f2fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "loader=ArxivLoader(query=\"2304.10557\",load_max_docs=2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "054c6994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79134644",
   "metadata": {},
   "source": [
    "### Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9376e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation-p/anaconda3/envs/conda-env-3-12/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/workstation-p/anaconda3/envs/conda-env-3-12/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "loader=WikipediaLoader(query=\"AI\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be81189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Artificial intelligence', 'summary': 'Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI\\'s ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI\\'s long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}, page_content='Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI\\'s ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI\\'s long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\n\\n=== Reasoning and problem-solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\n\\n=== Knowledge representation ===\\n\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and ma'),\n",
       " Document(metadata={'title': '.ai', 'summary': '.ai is the Internet country code top-level domain (ccTLD) for Anguilla, a British Overseas Territory in the Caribbean. It is administered by the government of Anguilla.\\nIt is a popular domain hack with companies and projects related to the artificial intelligence industry (AI).\\nGoogle\\'s ad targeting treats .ai as a generic top-level domain (gTLD) because \"users and website owners frequently see [the domain] as being more generic than country-targeted.\"\\nIn 2021, Google Search analyst Gary Illyes announced that \".ai\" had been added to Google’s list of generic country-code top-level domains, meaning that Google would no longer infer Anguilla-specific targeting from the ccTLD.\\nIdentity Digital began managing the domain as of January 2025.', 'source': 'https://en.wikipedia.org/wiki/.ai'}, page_content='.ai is the Internet country code top-level domain (ccTLD) for Anguilla, a British Overseas Territory in the Caribbean. It is administered by the government of Anguilla.\\nIt is a popular domain hack with companies and projects related to the artificial intelligence industry (AI).\\nGoogle\\'s ad targeting treats .ai as a generic top-level domain (gTLD) because \"users and website owners frequently see [the domain] as being more generic than country-targeted.\"\\nIn 2021, Google Search analyst Gary Illyes announced that \".ai\" had been added to Google’s list of generic country-code top-level domains, meaning that Google would no longer infer Anguilla-specific targeting from the ccTLD.\\nIdentity Digital began managing the domain as of January 2025.\\n\\n\\n== Second and third level registrations ==\\nRegistrations within off.ai, com.ai, net.ai, and org.ai are available worldwide without restriction. From 15 September 2009, second level registrations within .ai are available to everyone worldwide.\\n\\n\\n== Registration ==\\nThe minimum registration term allowed for .ai domains is 2 through 10 years for registration and renewal, and a 2-year renewal for domain transfer. Identity Digital is the authority in charge of managing this extension. Registrations began on 16 February 1995. The limits on the number of characters used for the domain name are, at a minimum, from 1 to 3, depending on the registrar, and always at most 63 characters. The character set supported for .ai domain names includes A–Z, a–z, 0–9, and hyphen. As of November 2022, .ai domains cannot accommodate IDN characters. There are no requirements for registering a domain, including local and foreign residents.\\nA .ai domain can be suspended or revoked, if the domain is involved in illegal activity such as violating trademarks or copyrights. Usage must not violate the laws of Anguilla.\\nAnguilla uses the UDRP. Filing a UDRP challenge requires using one of the ICANN Approved Dispute Resolution Service Providers. If the domain is with an ICANN accredited registrar, they should work with the arbitrator. Usually this means either doing nothing or transferring a domain. .ai domains are transferable to any desired registrars as the registration of domain is done maintaining EPP.\\nThere used to be a whois.ai-based platform of expired domains in which those could be procured and auctioned every ten days through a standard online process. The last auctions of such kind closed there in December 2024; the platform had been scheduled for shutdown on 30 June 2025, but remained online in the months following that date.\\n\\n\\n== Valuation ==\\nDomains cost depends on the registrar, with yearly fees ranging from US$140 (the base fee, as established by Anguilla) to $200. As of July 2025, the highest-valued .ai domain is an undisclosed one sold on 8 November 2023, on Escrow.com, for US$1,500,000—months after an initial $300,000 sale to the same buyer. Among the publicly disclosed ones, the most valued, fin.ai, was sold for $1,000,000 in March 2025.\\nOn 16 December 2017, the .ai registry started supporting the Extensible Provisioning Protocol (EPP) and migrated all of its domains onto an EPP system. Consequently, many registrars are allowed to sell .ai domains. Since that date, the .ai ccTLD has also been popular with artificial intelligence companies and organizations. Though such trends are primarily seen among new AI based companies or startups, many established AI and Tech companies preferred not to opt for .ai domains. For example, DeepMind has its domain retained at .com; Meta has redirected its facebook.ai domain to ai.meta.com.\\n\\n\\n== Impact on Anguilla\\'s economy ==\\nThe registration fees earned from the .ai domains go to the treasury of the Government of Anguilla. As per a 2018 New York Times report, the total revenue generated out of selling .ai domains was $2.9 million.\\nIn 2023, Anguilla\\'s government made about US$32 million from fees collected for registering .ai domains; that amounted to over 10% of gross domes'),\n",
       " Document(metadata={'title': 'AI slop', 'summary': 'AI slop (known simply as slop) is digital content made with generative artificial intelligence that is lacking in effort, quality, or meaning, and produced in high volume as clickbait to gain advantage in the attention economy, or earn money. It is a form of synthetic media usually linked to the monetization in the creator economy of social media and online advertising. Coined in the 2020s, the term has a pejorative connotation similar to spam. \"Slop\" was selected as the 2025 Word of the Year by both Merriam-Webster and the American Dialect Society.\\nAI slop has been variously defined as \"digital clutter\", \"filler content prioritizing speed and quantity over substance and quality\", and \"shoddy or unwanted AI content in social media, art, books, and search results\". Jonathan Gilmore, a philosophy professor at the City University of New York, describes the material as having an \"incredibly banal, realistic style\" that is easy for the viewer to process.', 'source': 'https://en.wikipedia.org/wiki/AI_slop'}, page_content='AI slop (known simply as slop) is digital content made with generative artificial intelligence that is lacking in effort, quality, or meaning, and produced in high volume as clickbait to gain advantage in the attention economy, or earn money. It is a form of synthetic media usually linked to the monetization in the creator economy of social media and online advertising. Coined in the 2020s, the term has a pejorative connotation similar to spam. \"Slop\" was selected as the 2025 Word of the Year by both Merriam-Webster and the American Dialect Society.\\nAI slop has been variously defined as \"digital clutter\", \"filler content prioritizing speed and quantity over substance and quality\", and \"shoddy or unwanted AI content in social media, art, books, and search results\". Jonathan Gilmore, a philosophy professor at the City University of New York, describes the material as having an \"incredibly banal, realistic style\" that is easy for the viewer to process.\\n\\n\\n== Origin of the term ==\\nAs early large language models (LLMs) and image diffusion models accelerated the creation of high-volume but low-quality text and images, discussion commenced among journalists and on social platforms for the appropriate term for the influx of material. Terms proposed included \"AI garbage\", \"AI pollution\", and \"AI-generated dross\". Early uses of the term  \"slop\" as a descriptor for low-grade AI material apparently came in reaction to the release of AI image generators in 2022. Its early use has been noted among 4chan, Hacker News, and YouTube commentators as a form of in-group slang.\\nThe British computer programmer Simon Willison is credited with being an early champion of the term \"slop\" in the mainstream, having used it on his personal blog in May 2024. However, he has said it was in use long before he began pushing for the term.\\nThe term gained increased popularity in the second quarter of 2024 in part because of Google\\'s use of its Gemini AI model to generate responses to search queries, and the large quantities of slop on the internet were widely criticized in media headlines during the fourth quarter of 2024.\\n\\n\\n== Definitions ==\\nAccording to an academic article by Cody Kommers and five other scholars that was published in January 2026, AI slop has \"so far resisted formal definition.\" Although they argue it is impossible to precisely describe a boundary between slop and non-slop, Kommers et.al. identify three \"prototypical properties\" that characterise AI slop: superficial competence, asymmetric effort and mass producibility. \\nBeyond these family resemblances, there are many different kinds of AI slop. Three main \"dimensions of variance\", or ways in which AI slop can vary, are its instrumental utility (why was it created?), the level of personalization (is it so specific as to only be interesting to one person or a small friend group?) and the level of surrealism, where some AI slop is \"ludicrously implausible\" while other slop is more realistic.\\nItalian artist and writer Francesco D\\'Isa argues that the production of mediocre, boring, and derivative works of art is not an exclusive trait of artificial intelligence but one shared by all forms of culture. He points out that for each work of art generally considered a masterpiece, there are many forgotten and unremarkable works, and that classics are only exceptions that happened to prevail. He has stated that \"the majority of human production has always been slop. Mediocrity is not a bug of technology; it is the baseline of culture.\" He also argues that the fear of AI is merely the first step in a typical trend for technological advances in media starting with panic, continuing to adaptation, and ending with incorporation into the cultural norm.\\n\\n\\n== On social media ==\\n\\nAI image and video slop have proliferated on social media in part because it can be revenue-generating for its creators on Facebook and TikTok, with the issue affecting Facebook most notably. This incentivizes individuals from developin'),\n",
       " Document(metadata={'title': 'AI bubble', 'summary': 'The AI bubble is a theorised stock market bubble growing amidst the AI boom, a period of rapid increase in investment in artificial intelligence (AI) that is affecting the broader economy. Speculation about a bubble largely originates from concerns that leading AI tech firms are involved in a circular flow of investments that are artificially inflating the value of their stocks. It has also drawn comparisons between the current environment of tech financing and the dot-com bubble of the 1990s and 2000s.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/AI_bubble'}, page_content='The AI bubble is a theorised stock market bubble growing amidst the AI boom, a period of rapid increase in investment in artificial intelligence (AI) that is affecting the broader economy. Speculation about a bubble largely originates from concerns that leading AI tech firms are involved in a circular flow of investments that are artificially inflating the value of their stocks. It has also drawn comparisons between the current environment of tech financing and the dot-com bubble of the 1990s and 2000s.\\n\\n\\n== History ==\\n\\nIn late January 2025, the unexpectedly successful launch of the Chinese-made chatbot DeepSeek resulted in concerns about a possible AI bubble. The stock prices of many AI companies dropped, such as Nvidia\\'s shares dropping 17% in one day. Nvidia\\'s share price recovered 8.8% the following day. In August 2025, a report by Nanda (Networked Agents and Decentralized AI), under Massachusetts Institute of Technology\\'s MIT Media Lab stated \"despite $30–40 billion [USD] in enterprise investment into Gen[erative]AI, [...] 95% of organizations are getting zero return\". Spending from US mega caps is expected to reach $1.1 trillion between 2026 and 2029, and total AI spending is expected to surpass $1.6 trillion.\\nDue to the growing demand for semiconductors to sustain AI technologies, Nvidia became the highest valued company in the world and the first to ever have reached a market value of $4 trillion in July 2025. The figure had quadrupled since 2023, when it surpassed $1 trillion. The company\\'s value made up roughly 7.3% of the S&P 500, which hit an all-time high. In October 2025, the company\\'s value grew beyond $5 trillion, rising higher than the GDP of every country except for the US and China, according to data from the World Bank. Over the year 2025, AI-related enterprises accounted for roughly 80% of gains in the American stock market. Some sceptics warned that the rapid rise of AI tech firms may be the result of excessive financial engineering.\\nMicrosoft disclosed that it had spent almost $35 billion on AI infrastructure in the three months leading up to the end of September. In October, it became the second most valuable company in the world largely due to its 27% stake in OpenAI. While seeing increases in revenue by 18% and in net income by 12%, share values dropped by 4% in after-hours trading amid investors\\' concerns about the possible costs of sustaining the AI boom.\\nIn late 2025, 30% of the US S&P 500 and 20% of the MSCI World index was solely held up by the five largest companies, which was the greatest concentration in half a century, and share valuations were reportedly the most stretched since the dot-com bubble. Experts warned that AI companies were extremely overvalued, with the S&P 500 trading at 23 times forward earnings, and the FTSE Index trading at 14 times, showing how expensive the US market had become. The Case–Shiller price-to-earnings ratio for the US market also exceeded 40 for the first time since the dot-com crash.\\n\\n\\n== Speculation ==\\n\\nSam Altman, CEO of OpenAI and creator of ChatGPT, stated in 2025 that he believed that an AI bubble is ongoing. In early 2025, Bridgewater Associates co-chief investment officer Ray Dalio said that the current levels of investment in AI are \"very similar\" to the dot-com bubble. In September 2025, the Australian Financial Review said that \"If we really are in another share-market bubble, it\\'s surely the most anticipated example in history.\"\\nIn October of that year, Jamie Dimon, head of JP Morgan, the largest bank in the US, said he thinks \"AI is real\" but said he believes some money invested now will be wasted. He also said there is a higher chance of a meaningful drop in stocks over the following two years than the market was reflecting. Dimon warned that an AI-driven stock crash could result in a lot of invested money being lost, although he acknowledged that AI \"[would] pay off […] just like cars in total paid off, and TVs in total paid off, but most people '),\n",
       " Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content=\"Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== Functions and production strategies ==\\nAI pornography platforms, beyond account creation and social media linking, primarily enable users to generate sexual images through feature selection or text prompting. Users can customize bodies, clothing, and sociodemographic traits, and browse categorized galleries of user‑generated content. Several sites also support short pornographic videos or GIFs and modification tools such as nudifiers, deepfakes, and facemorphing. Platforms often allow fine‑tuning of parameters such as settings, style, or theme, and provide prompt enhancers or suggestions to improve outputs. Users may edit generated images, refine prior prompts, modify others’ work, or upload personal material as a basis, with iterative and collaborative content creation. Some websites additionally host interactive “erobots,” customizable in real time for appearance, personality, memories, speech, and profession, enabling tailored sexual and non‑sexual interactions. Less common features include VR integration, AI porn games, audio or doodle prompts, and consensual replication of individuals with verification.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI's release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI's warnings against sexual imagery, SD's public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\n\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Conse\"),\n",
       " Document(metadata={'title': 'AI agent', 'summary': 'In the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require continuous oversight.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/AI_agent'}, page_content='In the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require continuous oversight.\\n\\n\\n== Overview ==\\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\\nAI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S..\\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user\\'s prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY, Gibberlink, the Internet of Agents, Agent2Agent (by Google), and the Agent Network Protocol. Some of these protocols are also used for connecting agents with external applications. Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.\\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research. Hugging Face also released a free web browser agent, similar to OpenAI Operator. Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\\nIn December 2025, Linux Foundation announced the formation of the Agentic AI Foundation (AAIF) - a neutral, open foundation to ensure agentic AI evolves transparently and collaboratively.\\nMemory systems for agents include Mem0, MemGPT, and MemOS.\\n\\n\\n== History ==\\n\\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of an AI agent was not clear at the time either. Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024.\\n\\n\\n== Training and testing ==\\nResearchers have attempted to build world models and reinforcement learning environments to train or evaluate AI agents. For example, video games such as Minecraft and No Man\\'s Sky as well as replicas of company websites, have also been used for training AI agents.\\n\\n\\n== Autonomous capabilities ==\\nThe Financial Times compared the autonomy of AI agents to the SAE classification of self-driving cars, comparing most applications to level 2 or level 3, with some achieving level 4 in highly specialized circumstances, and level 5 being theoretical.\\n\\n\\n== Cognitive architecture ==\\n\\nThe following are some possible internal design options for reasoning within an agent:\\n\\nRetrieval-augmented generation\\nReAct (Reason + Act) pattern is an iterative process in which an AI agent alternates between reasoning and taking actions, receives observations from the environment or external tools, and integrates these observations into subsequent reasoning steps.\\nReflexion, which uses an LLM to create feedback on the agent\\'s plan of action and stores that feedback in a memory cache.\\nA tool/agent registry, for organ'),\n",
       " Document(metadata={'title': 'Perplexity AI', 'summary': 'Perplexity AI, Inc., or simply Perplexity, is an American privately held software company offering a web search engine that processes user queries and synthesizes responses. Perplexity products use large language models and incorporate real-time web search capabilities, providing responses based on current Internet content, citing sources used. A free public version is available, while a paid Pro subscription offers access to more advanced language models and additional features.\\nPerplexity AI, Inc., was founded in 2022 by Aravind Srinivas, Denis Yarats, Johnny Ho, and Andy Konwinski. As of September 2025, the company was valued at US$20 billion.\\nPerplexity AI has attracted legal scrutiny over allegations of copyright infringement, unauthorized content use, and trademark issues from several major media organizations, including the BBC, Dow Jones, and The New York Times. According to separate analyses by Wired and later Cloudflare, Perplexity uses undisclosed web crawlers with spoofed user-agent strings to scrape the content of websites which prohibit, or explicitly block, web scraping.', 'source': 'https://en.wikipedia.org/wiki/Perplexity_AI'}, page_content='Perplexity AI, Inc., or simply Perplexity, is an American privately held software company offering a web search engine that processes user queries and synthesizes responses. Perplexity products use large language models and incorporate real-time web search capabilities, providing responses based on current Internet content, citing sources used. A free public version is available, while a paid Pro subscription offers access to more advanced language models and additional features.\\nPerplexity AI, Inc., was founded in 2022 by Aravind Srinivas, Denis Yarats, Johnny Ho, and Andy Konwinski. As of September 2025, the company was valued at US$20 billion.\\nPerplexity AI has attracted legal scrutiny over allegations of copyright infringement, unauthorized content use, and trademark issues from several major media organizations, including the BBC, Dow Jones, and The New York Times. According to separate analyses by Wired and later Cloudflare, Perplexity uses undisclosed web crawlers with spoofed user-agent strings to scrape the content of websites which prohibit, or explicitly block, web scraping.\\n\\n\\n== History ==\\nIn August 2022, Perplexity AI, Inc., was founded by Aravind Srinivas, Denis Yarats, Johnny Ho, and Andy Konwinski, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning. It launched its main search engine on December 7, 2022, and has since released a Google Chrome extension and apps for iOS and Android. In February 2023, Perplexity reported two million unique visitors. By April 2024, Perplexity had raised $165 million in funding, valuing the company at over $1 billion. As of June 2025, Perplexity closed a $500 million round of funding that elevated its valuation to $14 billion. Investors in Perplexity AI have included Jeff Bezos, Tobias Lütke, Nat Friedman, Nvidia, and Databricks. During Bloomberg’s Tech Summit 2025, Srinivas shared that the company processed 780 million queries in May 2025, experiencing more than 20% month-over-month growth, processing around 30 million queries daily.\\nIn July 2024, Perplexity announced the launch of a new publishers\\' program to share advertising revenue with partners.\\nOn January 18, 2025, the day before the impending U.S. ban on the social media app TikTok, Perplexity submitted a proposal for a merger with TikTok US.\\nOn August 12, 2025, Perplexity made a bid to buy Chrome from Google for $34.5 billion. Perplexity stated that the sale could remedy anti-trust litigation against Google, in which a judge was considering compelling the sale of Chrome. \\nOn December 08, 2025, Cristiano Ronaldo invested in Perplexity AI, linking his global CR7 brand with search capabilities with a goal to expand AI access and visibility worldwide.\\n\\n\\n== Products and services ==\\n \\n\\n\\n=== Search engine web portal ===\\nPerplexity’s primary offering is an online information retrieval system (search engine) that uses large language models to generate responses to user queries by searching and summarizing web-based content. Perplexity offers a feature known as Perplexity Pages that generates structured summaries and report-like content from user queries by aggregating cited sources. Perplexity is available without charge or registration to Web users, a freemium model.\\n\\n\\n=== Perplexity Pro ===\\nPerplexity Pro is a subscription tier, a more capable paid \"enterprise\" service, including stronger security and data protection and additional tools, including the ability to search uploaded documents alongside web content and access to a programmatic application programming interface (API). It allows the user to select between backend models such as GPT-5, GPT-4.1, o4-mini, Claude 4.0, Grok 4 and Gemini Pro 3. The company has also developed its own models, Sonar (based on Llama 3.3) and R1 1776 (based on DeepSeek R1).\\n\\n\\n=== Internal Knowledge Search ===\\nInternal Knowledge Search enables Pro and Enterprise Pro users to simultaneously search across web content and internal documents. Users can upl'),\n",
       " Document(metadata={'title': 'Generative artificial intelligence', 'summary': \"Generative artificial intelligence, also known as generative AI or GenAI, is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data, and use them to generate new data in response to input, which often takes the form of natural language prompts.\\nThe prevalence of generative AI tools has increased significantly since the AI boom in the 2020s. This boom was made possible by improvements in deep neural networks, particularly large language models (LLMs), which are based on the transformer architecture. Generative AI applications include chatbots such as ChatGPT, Claude, Copilot, DeepSeek, Google Gemini and Grok; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTX and Sora.\\nCompanies in a variety of sectors have used generative AI, including those in software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, and product design.  \\nGenerative AI has been used for cybercrime, and to deceive and manipulate people through fake news and deepfakes. Generative AI models have been trained on copyrighted works without the rightholders' permission. Many generative AI systems use large-scale data centers whose environmental impacts include e-waste, consumption of fresh water for cooling, and high energy consumption that is estimated to be growing steadily.\", 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence, also known as generative AI or GenAI, is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data, and use them to generate new data in response to input, which often takes the form of natural language prompts.\\nThe prevalence of generative AI tools has increased significantly since the AI boom in the 2020s. This boom was made possible by improvements in deep neural networks, particularly large language models (LLMs), which are based on the transformer architecture. Generative AI applications include chatbots such as ChatGPT, Claude, Copilot, DeepSeek, Google Gemini and Grok; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTX and Sora.\\nCompanies in a variety of sectors have used generative AI, including those in software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, and product design.  \\nGenerative AI has been used for cybercrime, and to deceive and manipulate people through fake news and deepfakes. Generative AI models have been trained on copyrighted works without the rightholders\\' permission. Many generative AI systems use large-scale data centers whose environmental impacts include e-waste, consumption of fresh water for cooling, and high energy consumption that is estimated to be growing steadily.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe origins of algorithmically generated media can be traced to the development of the Markov chain, which has been used to model natural language since the early 20th century. Russian mathematician Andrey Markov introduced the concept in 1906, including an analysis of vowel and consonant patterns in Eugeny Onegin. Once trained on a text corpus, a Markov chain can generate probabilistic text.\\nBy the early 1970s, artists began using computers to extend generative techniques beyond Markov models. Harold Cohen developed and exhibited works produced by AARON, a pioneering computer program designed to autonomously create paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (since the late 2000s) ===\\n\\nMachine learning uses both discriminative models and generative models to predict data. Beginning in the late 2000s, the introduction of deep learning technology led to improvements in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images, such as DeepDream.\\nIn 2017, the Transformer network enabled advancements in generative models compared to older long short-term memory (LSTM) models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.\\n\\n\\n=== Generative AI adoption ===\\n\\nIn March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing'),\n",
       " Document(metadata={'title': 'XAI (company)', 'summary': \"X.AI Corp., doing business as xAI, is an American company working in the area of artificial intelligence (AI), social media and technology that is a wholly owned subsidiary of American aerospace company SpaceX. Founded by Elon Musk in 2023, the company's flagship products are the generative AI chatbot named Grok and the social media platform X (formerly Twitter), the latter of which they acquired in March 2025.\", 'source': 'https://en.wikipedia.org/wiki/XAI_(company)'}, page_content='X.AI Corp., doing business as xAI, is an American company working in the area of artificial intelligence (AI), social media and technology that is a wholly owned subsidiary of American aerospace company SpaceX. Founded by Elon Musk in 2023, the company\\'s flagship products are the generative AI chatbot named Grok and the social media platform X (formerly Twitter), the latter of which they acquired in March 2025.\\n\\n\\n== History ==\\nxAI was founded on March 9, 2023, by Musk. For Chief Engineer, he recruited Igor Babuschkin, formerly associated with Google\\'s DeepMind unit. Musk officially announced the formation of xAI on July 12, 2023.\\nAs of July 2023, xAI was headquartered in the San Francisco Bay Area. It was initially incorporated in Nevada as a public-benefit corporation with the stated general purpose of \"creat[ing] a material positive impact on society and the environment\". By May 2024, it had dropped the public-benefit status. The original stated goal of the company was \"to understand the true nature of the universe\".\\n\\n\\n=== Financial history ===\\nIn November 2023, Musk stated that \"X Corp investors will own 25% of xAI\". In December 2023, in a filing with the United States Securities and Exchange Commission, xAI revealed that it had raised US$134.7 million in outside funding out of a total of up to $1 billion. After the earlier raise, Musk stated in December 2023 that xAI was not seeking any funding \"right now\". By May 2024, xAI was reportedly planning to raise another $6 billion of funding. Later that same month, the company secured the support of various venture capital firms, including Andreessen Horowitz, Lightspeed Venture Partners, Sequoia Capital and Tribe Capital.\\nAs of August 2024, Musk was diverting a large number of Nvidia chips that had been ordered by Tesla, Inc. to X and xAI. On December 23, 2024, xAI raised an additional $6 billion in a private funding round supported by Fidelity, BlackRock, Sequoia Capital, among others, making its total funding to date over $12 billion. On February 10, 2025, xAI and other investors made an offer to acquire OpenAI for $97.4 billion. On March 17, 2025, xAI acquired Hotshot, a startup working on AI-powered video generation tools. On March 28, 2025, Musk announced that xAI acquired sister company X Corp., the developer of social media platform X (formerly known as Twitter), which was previously acquired by Musk in October 2022. The deal, an all-stock transaction, valued X at $33 billion, with a full valuation of $45 billion when factoring in $12 billion in debt. Meanwhile, xAI itself was valued at $80 billion. Both companies were combined into a single entity called X.AI Holdings Corp.\\nOn July 1, 2025, Morgan Stanley announced that they had raised $5 billion in debt for xAI and that xAI had separately raised $5 billion in equity. The debt consists of secured notes and term loans. Morgan Stanley took no stake in the debt. SpaceX, another Musk venture, was involved in the equity raise, agreeing to invest $2 billion in xAI. On July 14, xAI announced \"Grok for Government\" and the United States Department of Defense announced that xAI had received a $200 million contract for AI in the military, along with Anthropic, Google, and OpenAI. On September 12, xAI laid off 500 data annotation workers. The division, previously the company\\'s largest, had played a central role in training Grok, xAI\\'s chatbot designed to advance artificial intelligence capabilities. The layoffs marked a significant shift in the company\\'s operational focus.\\nOn November 26, 2025, Elon Musk announced his plans to build a solar farm near Colossus with an estimated output of 30 megawatts of electricity, which is 10% of the data center\\'s estimated power use. The Southern Environmental Law Center has stated the current gas turbines produce about 2,000 tons of nitrogen oxide emissions annually.\\n\\n\\n=== Environmental impact ===\\nIn June 2024, the Greater Memphis Chamber announced xAI was planning on building Colossus, the worl'),\n",
       " Document(metadata={'title': 'AI boom', 'summary': 'An AI boom is a period of rapid growth in the field of artificial intelligence (AI). The most recent boom originally started gradually in the 2010s, but saw increased acceleration in the 2020s. Examples of this include generative AI technologies, such as large language models and AI image generators developed by companies like OpenAI, as well as scientific advances, such as protein folding prediction led by Google DeepMind. This period is sometimes referred to as an AI spring, a term used to differentiate it from previous AI winters. As of 2025, ChatGPT has emerged as the 4th-most visited website globally, surpassed only by Google, YouTube, and Facebook.', 'source': 'https://en.wikipedia.org/wiki/AI_boom'}, page_content='An AI boom is a period of rapid growth in the field of artificial intelligence (AI). The most recent boom originally started gradually in the 2010s, but saw increased acceleration in the 2020s. Examples of this include generative AI technologies, such as large language models and AI image generators developed by companies like OpenAI, as well as scientific advances, such as protein folding prediction led by Google DeepMind. This period is sometimes referred to as an AI spring, a term used to differentiate it from previous AI winters. As of 2025, ChatGPT has emerged as the 4th-most visited website globally, surpassed only by Google, YouTube, and Facebook.\\n\\n\\n== History ==\\n\\nIn 1950, Alan Turing proposed the idea of \"Thinking Machines\". These were computers that would be able to reason at the same level as humans. He began his well-known \"Turing Test\", where an interrogator is provided with two materials and they must determine which one was done by artificial intelligence and which one was done by a human being. In 1956, John McCarthy used the term \"artificial intelligence\" for the first time, eventually being labeled as the father of artificial intelligence.\\nIn 1956, the Dartmouth conference was held, organized by John McCarthy, Nathaniel Rochester, Marvin Minsky, and Claude Shannon. This conference is considered the birthplace of artificial intelligence as a field of study, as a workshop was held for 2 months. During this workshop, top researchers explored the concept of creating machines that could mimic the same intelligence as human beings.\\n\\nIn 1958, John McCarthy created the programming language LISP. LISP stands for \"List Processing\" and works as the main programming language for artificial intelligence. The programming language gained traction at MIT, being used for many of their projects that dealt with AI, such as the IBM 704. While many languages rose and fell, LISP remained the most common programming language for artificial intelligence in the United States even in 2006. LISP became so reliable due to how artificial intelligence works. Artificial intelligence of the time often had lists that constantly change size, making fixed-length methods, such as vectors, unusable.\\nIn 1962, in order to continue research on artificial intelligence, John McCarthy founded Stanford Artificial Intelligence Laboratory (SAIL). SAIL became an important hub for AI research, helping contribute many advancements in the field. Some of these advancements include robotics, medical diagnostics, natural language processing, autonomous vehicles, and more. John McCarthy was also a cofounder of MIT\\'s first Artificial Intelligence Laboratory, now known as MIT Computer Science and Artificial Intelligence Laboratory.\\nIn 1966, Joseph Weizenbaum created ELIZA. ELIZA was designed to be an emotional tool, being considered a \"Rogerian psychotherapist\". This was done by making it seem like the chatbot reflected on the user\\'s input, turning questions back to the user. ELIZA is known as the first artificial intelligence chatbot. ELIZA uses strategies such as pattern matching and substitution in order to provide outputs that make users believe they are talking to a real person. Weizenbaum\\'s ELIZA was a huge advancement for regular use AI, acting as a building block for future chatbots such as OpenAI\\'s ChatGPT or Google\\'s Gemini.\\nArtificial intelligence began being added to new devices. A popular implementation of artificial intelligence would be AI assistants. In 2011, Apple released the iPhone 4S. This new smartphone would include a new AI assistant named Siri, originally developed by Dag Kittlaus, Adam Cheyer, and Tom Gruber in 2007. Originally owning their own company, Siri Inc., Apple saw the potential in the assistant and chose to integrate it into their new iOS. Siri was revolutionary, acting as the first mainstream smartphone AI assistant. Navigating or setting tasks became way simpler, only needing to use your voice for a hands-free approach to intera'),\n",
       " Document(metadata={'title': 'Mistral AI', 'summary': 'Mistral AI SAS (French: [mistʁal]) is a French artificial intelligence (AI) company, headquartered in Paris. Founded in 2023, it has open-weight large language models (LLMs), with both open-source and proprietary AI models. As of 2025 the company has a valuation of more than US$14 billion.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Mistral_AI'}, page_content='Mistral AI SAS (French: [mistʁal]) is a French artificial intelligence (AI) company, headquartered in Paris. Founded in 2023, it has open-weight large language models (LLMs), with both open-source and proprietary AI models. As of 2025 the company has a valuation of more than US$14 billion.\\n\\n\\n== Namesake ==\\nThe company is named after the mistral, a powerful, cold wind in southern France, a term which originates from the Occitan language.\\n\\n\\n== History ==\\nMistral AI was established in April 2023 by three French AI researchers, Arthur Mensch, Guillaume Lample and Timothée Lacroix.\\nMensch, an expert in advanced AI systems, is a former employee of Google DeepMind; Lample and Lacroix, meanwhile, are large-scale AI models specialists who had worked for Meta Platforms.\\nThe trio originally met during their studies at École Polytechnique.\\n\\n\\n== Company operation ==\\n\\n\\n=== Funding ===\\nIn June 2023, the start-up carried out a first fundraising of €105 million ($117 million) with investors including the American fund Lightspeed Venture Partners, Eric Schmidt, Xavier Niel and JCDecaux. The valuation was then estimated by the Financial Times at €240 million ($267 million).\\nOn 10 December 2023, Mistral AI announced that it had raised €385 million ($428 million) as part of its second fundraising. This round of financing involves the Californian fund Andreessen Horowitz, BNP Paribas and the software publisher Salesforce.\\nBy December 2023, it was valued at over $2 billion.\\nOn 16 April 2024, reporting revealed that Mistral was in talks to raise €500 million, a deal that would more than double its current valuation to at least €5 billion.\\nIn June 2024, Mistral AI secured a €600 million ($645 million) funding round, increasing its valuation to €5.8 billion ($6.2 billion). Based on valuation, as of June 2024, the company was ranked fourth globally in the AI industry, and first outside the San Francisco Bay Area.\\nIn August 2025, the Financial Times reported that Mistral was in talks to raise $1 billion at a $10 billion valuation. In September 2025, Bloomberg announced that Mistral AI has secured a €2 billion investment valuing it at €12 billion ($14 billion). This comes after $1.5 billion investment from Dutch company ASML, which owns 11% of Mistral.\\n\\n\\n=== Partnerships ===\\nOn 26 February 2024, Microsoft announced that Mistral\\'s language models would be made available on Microsoft\\'s Azure cloud, while the multilingual conversational assistant Le Chat would be launched in the style of ChatGPT. The partnership also included a financial investment of $16 million by Microsoft in Mistral AI.\\nIn April 2025, Mistral AI announced a €100 million partnership with the shipping company CMA CGM.\\n\\n\\n== Services ==\\nOn 19 November, 2024, the company announced updates for Le Chat (pronounced /lə ʃa/ in French, like the French word for \"cat\"). It added the ability to create images, using Black Forest Labs\\' Flux Pro model.\\nOn 6 February 2025, Mistral AI released Le Chat on iOS and Android mobile devices.\\nMistral AI also introduced a Pro subscription tier, priced at $14.99 per month, which provides access to more advanced models, unlimited messaging, and web browsing.\\n\\n\\n== Models ==\\nThe following table lists the main model versions of Mistral, describing the significant changes included with each version:\\n\\n\\n=== Mistral 7B ===\\nMistral AI claimed in the Mistral 7B release blog post that the model outperforms LLaMA 2 13B on all benchmarks tested, and is on par with LLaMA 34B on many benchmarks tested, despite having only 7 billion parameters, a small size compared to its competitors.\\n\\n\\n=== Mixtral 8x7B ===\\nMistral AI claimed in 2023 that its model beat both LLaMA 70B, and GPT-3.5 in most benchmarks.\\nIn March 2024, research conducted by Patronus AI comparing performance of LLMs on a 100-question test with prompts to generate text from books protected under U.S. copyright law found that OpenAI\\'s GPT-4, Mixtral, Meta AI\\'s LLaMA-2, and Anthropic\\'s Claude 2 generated copyrighted tex'),\n",
       " Document(metadata={'title': 'Z.ai', 'summary': 'Knowledge Atlas Technology Joint Stock Co., Ltd. branded internationally as Z.ai, is a Chinese technology company specializing in artificial intelligence (AI). The company was formerly known as Zhipu AI outside China until its rebranding in 2025.\\nAs of 2024, it is one of China\\'s \"AI Tiger\" companies by investors and considered to be the third largest LLM market player in China\\'s AI industry according to the International Data Corporation. In January 2025, the United States Commerce Department blacklisted the company in its Entity List due to national security concerns.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Z.ai'}, page_content='Knowledge Atlas Technology Joint Stock Co., Ltd. branded internationally as Z.ai, is a Chinese technology company specializing in artificial intelligence (AI). The company was formerly known as Zhipu AI outside China until its rebranding in 2025.\\nAs of 2024, it is one of China\\'s \"AI Tiger\" companies by investors and considered to be the third largest LLM market player in China\\'s AI industry according to the International Data Corporation. In January 2025, the United States Commerce Department blacklisted the company in its Entity List due to national security concerns.\\n\\n\\n== History ==\\n\\nFounded in 2019, the startup company began from Tsinghua University and was later spun out as an independent company.\\nIn 2023, it raised 2.5 billion yuan (approx. 350 million in USD) from Alibaba Group and Tencent, along with Meituan, Ant Group, Xiaomi, and HongShan. In March 2024, Zhipu AI announced it was developing a Sora-like technology to achieve artificial general intelligence (AGI). In May 2024, the Saudi Arabian finance firm Prosperity7 Ventures, LLC participated in a USD $400 million financing round for Zhipu AI with a valuation of approximately 3 billion USD. In July 2024, they debuted the Ying text-to-video model.\\nIn October 2024, Zhipu AI released GLM-4.0, an open-source end-to-end speech large language model. The model can replicate human-like interactions and has the capability to adjust its tone, emotion, or dialect based from user\\'s preference. In May 2025, the company sealed a 61.28 million yuan deal from the Chinese government for city projects in Hangzhou.\\nIn July 2025, Zhipu AI released GLM-4.5 and GLM-4.5 Air, their next generation language models, and the company rebranded itself as Z.ai internationally. In August 2025, Z.ai announced that their GLM models are compatible with Huawei\\'s Ascend processors. On August 11, 2025, Z.ai released a new vision-language model (VLM) with a total of 106B parameters, GLM-4.5V. In late September 2025, the company released GLM 4.6 using China\\'s domestic chips such as those from Cambricon Technologies. That same year, the company had its official name to Knowledge Atlas Technology JSC Ltd. \\nOn 8 January 2026, Z.ai held its initial public offering on the Hong Kong Stock Exchange to become a listed company. It is considered to be China\\'s first major LLM company that went through an IPO. In February 2026, JPMorgan Chase recommended to investors of purchasing stocks of the company alongside MiniMax.\\nOn February 11, 2026, zAI Released GLM-5.\\n\\n\\n== Description ==\\nZ.ai provides the following products and services:\\n\\nGLM (General Language Model, formerly known as ChatGLM): series of pre-trained dialogue models initially developed by Zhipu AI and Tsinghua KEG in 2023. GLM 4.5, released in July 2025 by Z.ai, can run at only eight NVIDIA H20 chips. The release of GLM 4.6 in late September 2025 marked the first integration of FP8 and Int4 quantization on Cambricon chips. It also supports native FP8 on Moore Threads GPUs.\\nYing: Text-to-video model that generates image and text prompts into a six-second video clip for around 30 seconds.\\nAutoGLM: AI agent application that uses voice commands to complete tasks within a smartphone. The app can analyze complex tasks such as ordering an item from a nearby store and repeating an order based from the user\\'s shopping history.\\nAMiner: Created by Jie Tang (co-founder of Z.ai) in March 2006, now owned by Z.ai.\\nZ.ai has offices in the Middle East, United Kingdom, Singapore, and Malaysia, along with innovation center projects across Southeast Asia (2025). In January 2025, the United States Commerce Department added the company to its Entity List, citing national security concerns.\\n\\n\\n== See also ==\\nBaichuan\\nMiniMax\\nMoonshot AI\\n\\n\\n== References ==\\n\\n\\n== Notes ==\\n\\n\\n== External links ==\\nOfficial website\\nChat with Z.ai\\nZ.ai on Hugging Face\\nZ.ai on GitHub'),\n",
       " Document(metadata={'title': 'Ai-Ai delas Alas', 'summary': 'Martina Eileen \"Ai-Ai\" Hernandez delas Alas-Sibayan (born November 11, 1964) is a Filipino actress and comedian. Referred to as the \"Queen of Comedy\" for her comedic talent, she is best known for her role as Ina Montecillo in the film series Ang Tanging Ina. Her accolades include two Star Awards for Movies, a FAMAS Award, a Metro Manila Film Festival Award, an ASEAN International Film Festival and Awards, and a Cinemalaya Independent Film Festival, including nominations for three Gawad Urian and three Luna Awards. Her films have collectively earned ₱2.41 billion, making her the highest grossing Filipino comedy actress of all time.', 'source': 'https://en.wikipedia.org/wiki/Ai-Ai_delas_Alas'}, page_content='Martina Eileen \"Ai-Ai\" Hernandez delas Alas-Sibayan (born November 11, 1964) is a Filipino actress and comedian. Referred to as the \"Queen of Comedy\" for her comedic talent, she is best known for her role as Ina Montecillo in the film series Ang Tanging Ina. Her accolades include two Star Awards for Movies, a FAMAS Award, a Metro Manila Film Festival Award, an ASEAN International Film Festival and Awards, and a Cinemalaya Independent Film Festival, including nominations for three Gawad Urian and three Luna Awards. Her films have collectively earned ₱2.41 billion, making her the highest grossing Filipino comedy actress of all time.\\n\\n\\n== Early and personal life ==\\nMartina Eileen Hernandez Delas Alas is the daughter of Rosendo delas Alas (1920–2008) and Gregoria delas Alas née Hernández (died December 30, 2013) from Brgy. Taliba, San Luis, Batangas, and was later adopted by her aunt along with her brother Allen Tabada, Justa delas Alas. She earned a mass communication degree from Far Eastern University in 1985.\\nShe was married to actor Miguel Vera, with whom she had two children; she also has a son from a previous relationship with Hernan Jeng Jeng Viola, a guitarist. Before acting, she worked as a sales assistant in a department store, and also performed as a stand-up comic in comedy bars around Metro Manila. On April 3, 2013, she married Jed Salang who was 20 years her junior. The marriage ended after a month, with delas Alas confirming their separation on May 19, 2013. Delas Alas confirmed through Instagram her engagement to Gerald Sibayan in April 2017; the two married in November 2017 at Christ the King Parish Church in Quezon City. In November 2024, the couple officially confirmed their separation.\\nShe is also cousins with Ogie Alcasid.\\n\\n\\n== Career ==\\n\\nDelas Alas started her career on GMA Network back in 1991, as a host of Saturday late-night musical variety show RSVP, with Dawn Zulueta and Ariel Ureta. Delas Alas co-hosted GMA noontime shows Lunch Date (1991–93), SST: Salo-Salo Together (1993–95) and Eat Bulaga! (2000). She was a member of the cast of GMA sitcoms Ibang Klase with Joey De Leon, Jessa Zaragoza, Mark Anthony Fernandez and Aiza Seguerra in 1997–98, and 1 for 3 with Vic Sotto and Rosanna Roces between 1999 and 2001.\\nDelas Alas has acted in several films, including Ang Tanging Ina from Star Cinema, which became the highest-grossing film of 2003 and the highest-grossing Filipino film until 2006. It was followed by more comedy films like Volta where she played a superhero, which spun off into a television series. In the noon-time television show MTB: Ang Saya Saya she was one of the lead hosts. In prime-time teleseries, she played an important role in the TV remake of Bituing Walang Ningning as Dorina\\'s mother. She has also acted in the films Ang Cute Ng Ina Mo, Ang Tanging Ina N\\'yong Lahat and Pasukob.\\nShe also released her first album on Star Music, entitled Ang Tanging Ina Nyo; this featured six novelty tunes. During the November 9 episode of ASAP 19, she premiered her song \"Nandito Lang Ako\", and revealed that she would be going under the stage name of ADA.\\n\\n\\n=== Return to GMA Network (2015–2025) ===\\nAfter almost two decades of working with ABS-CBN, delas Alas decided to return to GMA Network, signing a two-year exclusive contract. One of the deciding factors for her move was the lack of projects for her from the Kapamilya Network. Her last show on ABS-CBN was the remake of Dyesebel alongside Anne Curtis.\\nHer return was celebrated in the Sunday-noontime show Sunday All Stars, where she was warmly welcomed as the newest Kapuso. As her first project, Delas Alas joined the cast of GMA\\'s Telebabad series Let The Love Begin, in which she played the mother of Ruru Madrid. The show\\'s final episode aired on August 7, 2015.\\nDelas Alas is also one of the main hosts and comedians in the newest Sunday afternoon comedy variety show Sunday PinaSaya, alongside Marian Rivera and the comedy duo Jose Manalo and Wally Bayola. Su'),\n",
       " Document(metadata={'title': 'Artificial general intelligence', 'summary': 'Artificial general intelligence (AGI) is a type of artificial intelligence that matches or surpasses human capabilities across virtually all cognitive tasks.\\nBeyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin. Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming.\\nCreating AGI is a stated goal of AI technology companies such as OpenAI, Google, xAI, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\\nAGI is a common topic in science fiction and futures studies.\\nContention exists over whether AGI represents an existential risk. Some AI experts and industry figures have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_general_intelligence'}, page_content='Artificial general intelligence (AGI) is a type of artificial intelligence that matches or surpasses human capabilities across virtually all cognitive tasks.\\nBeyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin. Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming.\\nCreating AGI is a stated goal of AI technology companies such as OpenAI, Google, xAI, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\\nAGI is a common topic in science fiction and futures studies.\\nContention exists over whether AGI represents an existential risk. Some AI experts and industry figures have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk.\\n\\n\\n== Terminology ==\\nAGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action.\\nSome academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness. In contrast, weak AI (or narrow AI) can solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.\\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.\\nA framework for classifying AGI was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).\\n\\n\\n== Characteristics ==\\n\\nPrior to the release of ChatGPT in November 2022, there was broad consensus on AGI as a theoretical benchmark for human-level machine intelligence. The capabilities demonstrated by GPT-3.5 and subsequent large language models challenged this framing directly, with some researchers and practitioners arguing that these systems already constitute AGI. The debate has since shifted from whether AGI is achievable to whether it has already been achieved and when exactly it occurred. OpenAI CEO Sam Altman, who initially maintained the pre-ChatGPT framing of AGI as a future milestone, conceded by December 2025 that \"we built AGIs\" and that \"AGI kinda went whooshing by,\" proposing the field move on to defining superintelligence. Computer scientist John McCarthy noted in 2007 the difficulty of characterising which computational procedures count as intelligent.\\n\\n\\n=== Intelligence traits ===\\nResearchers generally hold that a system is required to do all of the following to be regarded as an AGI:\\n\\nreason, use strategy, solve puzzles, and make judgments under uncertainty,\\nrepresent knowledge, including common sense knowledge,\\nplan,\\nlearn,\\ncommunicate in natural language,\\nif necessary, integrate these skills in completion of any given goal.\\nMany interdisciplinary approaches (e.g. cognitive science, computational intell'),\n",
       " Document(metadata={'title': 'Ai Weiwei', 'summary': 'Ai Weiwei (  EYE way-WAY; Chinese: 艾未未; pinyin: Ài Wèiwèi, IPA: [âɪ wêɪ.wêɪ]; born 28 August 1957) is a Chinese contemporary artist, documentarian, and activist. Ai grew up in the far northwest of China, where he lived under harsh conditions due to his father\\'s exile. As an activist, he has been openly critical of the Chinese government and its stance on democracy and human rights. He investigated government corruption and cover-ups, in particular the Sichuan schools corruption scandal following the collapse of \"tofu-dreg schools\" in the 2008 Sichuan earthquake. In April 2011, Ai Weiwei was arrested at Beijing Capital International Airport for \"economic crimes\", and detained for 81 days without charge. Ai Weiwei emerged as a figure in Chinese cultural development, an architect of Chinese modernism, and a prominent political commentator.\\nAi Weiwei has created political sculptures, photographs, and public works. Since being allowed to leave China in 2015, he has lived in Portugal, Germany, and the United Kingdom.', 'source': 'https://en.wikipedia.org/wiki/Ai_Weiwei'}, page_content='Ai Weiwei (  EYE way-WAY; Chinese: 艾未未; pinyin: Ài Wèiwèi, IPA: [âɪ wêɪ.wêɪ]; born 28 August 1957) is a Chinese contemporary artist, documentarian, and activist. Ai grew up in the far northwest of China, where he lived under harsh conditions due to his father\\'s exile. As an activist, he has been openly critical of the Chinese government and its stance on democracy and human rights. He investigated government corruption and cover-ups, in particular the Sichuan schools corruption scandal following the collapse of \"tofu-dreg schools\" in the 2008 Sichuan earthquake. In April 2011, Ai Weiwei was arrested at Beijing Capital International Airport for \"economic crimes\", and detained for 81 days without charge. Ai Weiwei emerged as a figure in Chinese cultural development, an architect of Chinese modernism, and a prominent political commentator.\\nAi Weiwei has created political sculptures, photographs, and public works. Since being allowed to leave China in 2015, he has lived in Portugal, Germany, and the United Kingdom.\\n\\n\\n== Life ==\\n\\n\\n=== Early life and work ===\\nAi\\'s father was the Chinese poet Ai Qing, who was denounced during the Anti-Rightist Movement. In 1958, the family was sent to a labour camp in Beidahuang, Heilongjiang, when Ai was one year old. They were subsequently exiled to Shihezi, Xinjiang, in 1961, where they lived for 16 years. Upon Mao Zedong\\'s death and the end of the Cultural Revolution, the family returned to Beijing in 1976.\\nWeiwei notes this exile as \"the whirlpool that swallowed up my father upended my life too, leaving a mark on me that I carry to this day.\"\\nIn 1978, Ai enrolled in the Beijing Film Academy and studied animation. In 1978, he was one of the founders of the early avant garde art group the \"Stars\", together with Ma Desheng, Wang Keping, Mao Lizi, Huang Rui, Li Shuang, Ah Cheng and Qu Leilei. The group disbanded in 1983, yet Ai participated in regular Stars group shows, The Stars: Ten Years, 1989 (Hanart Gallery, Hong Kong and Taipei), and a retrospective exhibition in Beijing in 2007: Origin Point (Today Art Museum, Beijing).\\n\\n\\n=== Life in the United States ===\\n\\nFrom 1981 to 1993, he lived in the United States. He was among the first generation of students to study abroad following China\\'s reform in 1980, being one of the 161 students to take the TOEFL (Test of English as a Foreign Language) in 1981. For the first few years, Ai lived in Philadelphia and San Francisco. He studied English at the University of Pennsylvania and the Berkeley Adult School. Later, he moved to New York City. He studied briefly at Parsons School of Design. Ai attended the Art Students League of New York from 1983 to 1986, where he studied with Bruce Dorfman, Knox Martin and Richard Pousette-Dart. He later dropped out of school and made a living out of drawing street portraits and working odd jobs. During this period, he gained exposure to the works of Marcel Duchamp, Andy Warhol, and Jasper Johns, and began creating conceptual art by altering readymade objects.\\nAi befriended beat poet Allen Ginsberg while living in New York, following a chance meeting at a poetry reading where Ginsberg read out several poems about China. Ginsberg had traveled to China and met with Ai\\'s father, the noted poet Ai Qing, and consequently Ginsberg and Ai became friends.\\nWhen he was living in the East Village (from 1983 to 1993), Ai carried a camera with him all the time and would take pictures of his surroundings wherever he was. The resulting collection of photos were later selected and is now known as the New York Photographs. At the same time, Ai became fascinated by blackjack card games and frequented Atlantic City casinos. He is still regarded in gambling circles as a top tier professional blackjack player according to an article published on blackjackchamp.com.\\n\\n\\n=== Return to China ===\\nIn 1993, Ai returned to China after his father became ill. He helped establish the experimental artists\\' Beijing East Village and co-published a series of '),\n",
       " Document(metadata={'title': 'Meta AI', 'summary': 'Meta AI is a research division of Meta (formerly Facebook) that develops artificial intelligence and augmented reality technologies.', 'source': 'https://en.wikipedia.org/wiki/Meta_AI'}, page_content='Meta AI is a research division of Meta (formerly Facebook) that develops artificial intelligence and augmented reality technologies.\\n\\n\\n== History ==\\nMeta AI was founded in 2013 as Facebook Artificial Intelligence Research (FAIR). It has workspaces in Menlo Park, London, New York City, Paris, Seattle, Pittsburgh, Tel Aviv, and Montreal as of 2025.\\nIn 2016, FAIR partnered with Google, Amazon, IBM, and Microsoft in creating the Partnership on Artificial Intelligence to Benefit People and Society.\\nMeta AI was directed by Yann LeCun until 2018, when Jérôme Pesenti succeeded the role. Pesenti is formerly the CTO of IBM\\'s big data group.\\nFAIR\\'s research includes self-supervised learning, generative adversarial networks, document classification and translation, and computer vision. FAIR released Torch deep-learning modules as well as PyTorch in 2017, an open-source machine learning framework, which was subsequently used in several deep learning technologies, such as Tesla\\'s autopilot  and Uber\\'s Pyro. That same year, a pair of chatbots were falsely rumored to be discontinued for developing a language that was unintelligible to humans. FAIR clarified that the research had been shut down because they had accomplished their initial goal to understand how languages are generated by their models, rather than out of fear.\\nFAIR was renamed Meta AI following the rebranding that changed Facebook, Inc. to Meta Platforms Inc.\\nOn October 1, 2025, Facebook announced \"We will soon use your interactions with AI at Meta to personalize the content and ads you see\".\\n\\n\\n== Virtual assistant ==\\nMeta AI is also the name of the virtual assistant developed by the team, now integrated as a chatbot into Meta\\'s social networking products. It is also available as a subscription-based stand-alone app.\\nThe virtual assistant was pre-installed on the second generation of Ray-Ban Meta smartglasses, and can incorporate inputs from the glasses\\' cameras after an update. It is also available on Quest 2 and newer HMDs.\\nSince May 2024, the chatbot has summarized news from various outlets without linking directly to original articles, including in Canada, where news links are banned on its platforms. This use of news content without compensation and attribution has raised ethical and legal concerns, especially as Meta continues to reduce news visibility on its platforms.\\n\\n\\n== Current research ==\\n\\n\\n=== Natural language processing and chatbot ===\\n\\nNatural language processing is the ability for machines to understand and generate natural language. The team is also researching unsupervised machine translation and multilingual chatbots.\\n\\n\\n==== Galactica ====\\nGalactica is a large language model (LLM) designed for generating scientific text. It was available for three days from 15 November 2022, before being withdrawn for generating racist and inaccurate content.\\n\\n\\n==== Llama ====\\n\\nLlama is an LLM released in February 2023. As of January 2026, the most recent release is the Llama 4.\\n\\n\\n=== Hardware ===\\nMeta used CPUs and in-house custom chips before 2022; they switched to Nvidia GPUs since then. MTIA v1, one of their early chips, is designed for the company\\'s content recommendation algorithms. It was fabricated on TSMC\\'s 7 nm process technology and consumed 25W, capable of 51.2 TFlops FP16.\\n\\n\\n== Controversy ==\\nThe French media outlet Mediapart reports that in 2022, Facebook\\'s parent company illegally used works accumulated by the pirate site LibGen to train its artificial intelligence.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website'),\n",
       " Document(metadata={'title': 'Character.ai', 'summary': 'character.ai (also known as c.ai, char.ai or Character AI) is a generative AI chatbot service where users can engage in conversations with customizable characters. It was designed by the developers of Google LaMDA, Noam Shazeer and Daniel de Freitas.\\nUsers can create \"characters\", craft their \"personalities\", set specific parameters, and then publish them to the community for others to chat with. Many characters are based on fictional media sources or celebrities, while others are original, some being made with certain goals in mind, such as assisting with creative writing, or playing a text-based adventure game.\\nThe beta version was made available to the public on September 16, 2022, and retired in September 2024, when it was replaced by the current website. In May 2023, a mobile app was released for iOS and Android, which received over 1.7 million downloads within a week.', 'source': 'https://en.wikipedia.org/wiki/Character.ai'}, page_content='character.ai (also known as c.ai, char.ai or Character AI) is a generative AI chatbot service where users can engage in conversations with customizable characters. It was designed by the developers of Google LaMDA, Noam Shazeer and Daniel de Freitas.\\nUsers can create \"characters\", craft their \"personalities\", set specific parameters, and then publish them to the community for others to chat with. Many characters are based on fictional media sources or celebrities, while others are original, some being made with certain goals in mind, such as assisting with creative writing, or playing a text-based adventure game.\\nThe beta version was made available to the public on September 16, 2022, and retired in September 2024, when it was replaced by the current website. In May 2023, a mobile app was released for iOS and Android, which received over 1.7 million downloads within a week.\\n\\n\\n== History ==\\nCharacter.ai was established in November 2021. The company\\'s co-founders, Noam Shazeer and Daniel de Freitas, were both engineers from Google. They both worked on AI-related projects: Shazeer was a lead author on a paper that Business Insider reported in April 2023 \"has been widely cited as key to today\\'s chatbots\", and Freitas was the lead designer of an experimental AI at Google initially called Meena, which later became known as LaMDA.\\nCharacter.ai raised $43 million in seed funding at the time of its initial foundation in 2021.\\nThe first beta version of Character.ai\\'s service was made available to the public on September 16, 2022. The Washington Post reported in October 2022 that the site had \"logged hundreds of thousands of user interactions in its first three weeks of beta-testing\". It allowed users to create their own new characters, and to play text-adventure game scenarios where users navigate scenarios described and managed by the chatbot characters.\\nFollowing a $150 million funding round in March 2023, Character.ai became valued at approximately $1 billion.\\nAs of January 2024, the site had 3.5 million daily visitors, the vast majority of them 16 to 30 years old.\\nIn 2024, Google hired Noam Shazeer, the CEO of Character.ai, and entered into a non-exclusive agreement to use Character.ai\\'s technology.\\n\\n\\n== Features ==\\n\\nCharacter.ai\\'s primary service is to let users converse with character AI chatbots based on fictional characters or real people (living or deceased). These characters\\' responses use data the chatbots gather from the internet about a person. In addition, users can play text-adventure games where characters guide them through scenarios. The company also provides a service that allows multiple users and AI chatbot characters to converse together at once in a single chatroom.\\nCharacter \"personalities\" are designed via descriptions from the point of view of the character and its greeting message, and further molded from conversations made into examples, giving its messages a star rating and modification to fit the precise dialect and identity the user desires.\\nWhen a character sends back a response, the user can rate the response from 1 to 4 stars. The rating predominantly affects the specific character, but also affects the behavioral selection as a whole.\\nOn May 11, 2023, Character.ai announced character.ai+, an opt-in subscription plan for $9.99 a month, that was marketed as including features such as skipping waiting rooms, fast messaging and responses, and access to an exclusion channel with faster support.\\nIn December 2024, amid multiple lawsuits and concerns, Character.ai introduced new safety features aimed at protecting teenage users. These enhancements include a dedicated model for users under 18, which moderates responses to sensitive subjects like violence and sex and has input and output filters to block harmful content. As a result of these changes and the deletion of custom-made bots flagged as violating the site\\'s terms, some users complained that the bots were too restrictive and lacked personality. The plat'),\n",
       " Document(metadata={'title': 'Moonshot AI', 'summary': 'Moonshot AI (Moonshot; Chinese: 月之暗面; pinyin: Yuè Zhī Ànmiàn; lit. \\'Dark Side of the Moon\\') is an artificial intelligence (AI) company based in Beijing, China. It has been dubbed one of China\\'s \"AI Tiger\" companies by investors with its focus on developing large language models.', 'source': 'https://en.wikipedia.org/wiki/Moonshot_AI'}, page_content='Moonshot AI (Moonshot; Chinese: 月之暗面; pinyin: Yuè Zhī Ànmiàn; lit. \\'Dark Side of the Moon\\') is an artificial intelligence (AI) company based in Beijing, China. It has been dubbed one of China\\'s \"AI Tiger\" companies by investors with its focus on developing large language models.\\n\\n\\n== Background ==\\nMoonshot was founded in March 2023 by Yang Zhilin, Zhou Xinyu and Wu Yuxin who were schoolmates at Tsinghua University. It was launched on the 50th anniversary of Pink Floyd\\'s The Dark Side of the Moon which was Yang\\'s favorite album and the inspiration for the company\\'s name.\\nYang has stated his goal for founding Moonshot AI is to build foundation models to achieve AGI. Yang\\'s three milestones are long context length, multimodal world model, and a scalable general architecture capable of continuous self-improvement without human input.\\nIn October 2023, the company released the first version of its chatbot, Kimi, which was capable of processing up to 200,000 Chinese characters per conversation.\\nIn June 2024, it was reported that Moonshot was planning to enter the US market. An insider revealed Moonshot was developing products for the US market, including an AI role-playing chat application called Ohai as well as a music video generator called Noisee. In response, Moonshot stated it had no plans to develop and release overseas products.\\nIn January 2026, Moonshot released Kimi K2.5, a multimodal upgrade to Kimi K2 that added native vision capabilities through a 400-million-parameter vision encoder called MoonViT. The model can process both images and video, enabling agentic tasks such as replicating website user journeys from video demonstrations alone. The K2 model was released just 3 months prior.\\n\\n\\n== Funding and investments ==\\nMoonshot was valued at $300 million when it received its initial funding of $60 million and had 40 employees.\\nIn February 2024, Alibaba Group led a $1 billion funding round for Moonshot, which gave it a valuation of $2.5 billion.\\nIn August 2024, Tencent and Gaorong Capital joined as investors in a $300 million funding round that valued Moonshot at $3.3 billion. \\nIn October 2025, Moonshot was reportedly nearing the completion of a new funding round of approximately $600 million, led by IDG Capital with participation from existing investors including Tencent, valuing the company at $3.8 billion pre-money.\\n\\n\\n== Products and research ==\\n\\n\\n=== Kimi ===\\n\\nIn October 2023, Moonshot launched its first AI chatbot, Kimi, whose name comes from Yang\\'s English nickname. It had emerged as the closest rival to Baidu\\'s Ernie Bot.\\nIn March 2024, Moonshot claimed Kimi could handle 2 million Chinese characters in a single prompt which was a significant upgrade from the previous version that could only handle 200,000. Due to the increased number of users, on 21 March, Kimi suffered an outage for two days and Moonshot had to issue an apology.\\nAs of August 2024, Kimi ranked third in active monthly users according to aicpb.com. \\nOn 20 January 2025, Kimi K1.5 was released. Moonshot claimed it matched the performance of OpenAI o1 in mathematics, coding, and multimodal reasoning capabilities.\\nIn June 2025, Kimi dropped in popularity to seventh place in active monthly users. \\nIn July 2025, the company released the weights for Kimi K2, a large language model with 1 trillion total parameters. The model uses a mixture-of-experts (MoE) architecture, where 32 billion parameters are active during inference. K2 was trained on 15.5 trillion tokens of data and is released under a modified MIT license. Kimi K2 is an open source LLM, meaning that it can be downloaded and built upon by users. The day after its release, Kimi K2 had the most downloads on the platform, an increase in popularity from previous months. Moonshot claims that the model excels in coding tasks, having passed tests like LiveCodeBench. In certain instances, the model performed on-par with or better than its Western counterparts. It has also been praised for its writing skills. '),\n",
       " Document(metadata={'title': 'Anthropic', 'summary': 'Anthropic PBC is an American artificial intelligence (AI) company headquartered in San Francisco. It has developed a family of large language models (LLMs) named Claude. The company researches and develops AI to \"study their safety properties at the technological frontier\" and use this research to deploy safe models for the public.\\nAnthropic was founded in 2021 by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei, who serve as president and CEO, respectively. As of February 2026, Anthropic has an estimated value of $380 billion.', 'source': 'https://en.wikipedia.org/wiki/Anthropic'}, page_content='Anthropic PBC is an American artificial intelligence (AI) company headquartered in San Francisco. It has developed a family of large language models (LLMs) named Claude. The company researches and develops AI to \"study their safety properties at the technological frontier\" and use this research to deploy safe models for the public.\\nAnthropic was founded in 2021 by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei, who serve as president and CEO, respectively. As of February 2026, Anthropic has an estimated value of $380 billion.\\n\\n\\n== History ==\\n\\n\\n=== Founding and early development (2021–2022) ===\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI\\'s Vice President of Research.\\nIn April 2022, Anthropic announced it had received $580 million in funding, including a $500 million investment from FTX under the leadership of Sam Bankman-Fried.\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.\\n\\n\\n=== Major investments ===\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority stakeholder by initially investing $1.25 billion and planning a total investment of $4 billion. The remaining $2.75 billion was invested in March 2024. In November 2024, Amazon invested another $4 billion, doubling its total investment. As part of the deal, Anthropic uses Amazon Web Services (AWS) as its primary cloud provider and makes its AI models available to AWS customers.\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time. In March 2025, Google agreed to invest another $1 billion in Anthropic.\\n\\n\\n=== Recruitment (2024) ===\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including Jan Leike, John Schulman, and Durk Kingma.\\n\\n\\n=== Additional funding and partnerships (2025) ===\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by Lightspeed Venture Partners with participation from several major investors. In March, Databricks and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features. It also introduced new API capabilities, including the Model Context Protocol (MCP) connector. The company hosted its inaugural developer conference that month. Also in May, Anthropic launched a web search API that enables Claude to access real-time information from the internet. Claude Code, Anthropic\\'s coding assistant, transitioned from research preview to general availability, featuring integrations with VS Code and JetBrains IDEs and support for GitHub Actions.\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by Iconiq Capital, Fidelity Management & Research, and Lightspeed Venture Partners, with participation from the Qatar Investment Authority and other investors. The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google\\'s custom Tensor Processing Units (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.\\nIn November 2025, Nvidia, Microsoft and Anthropic announced a partnership deal. NVIDIA and Microsoft were expected to invest up to $15 billion in'),\n",
       " Document(metadata={'title': 'OpenAI', 'summary': 'OpenAI is an American artificial intelligence research organization comprising both a non-profit foundation and a controlled for-profit public benefit corporation (PBC), headquartered in San Francisco. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". OpenAI is widely recognized for its development of the GPT family of large language models, the DALL-E series of text-to-image models, and the Sora series of text-to-video models, which have influenced industry research and commercial applications. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization was founded in 2015 in Delaware but evolved a complex corporate structure. As of October 2025, following restructuring approved by California and Delaware regulators, the non-profit OpenAI Foundation holds 26% of the for-profit OpenAI Group PBC, with Microsoft holding 27% and employees/other investors holding 47%.  Under its governance arrangements, the OpenAI Foundation holds the authority to appoint the board of the for-profit OpenAI Group PBC, a mechanism designed to align the entity’s strategic direction with the Foundation’s charter. Microsoft previously invested over $13 billion into OpenAI, and provides Azure cloud computing resources. In October 2025, OpenAI conducted a $6.6 billion share sale that valued the company at $500 billion.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.', 'source': 'https://en.wikipedia.org/wiki/OpenAI'}, page_content='OpenAI is an American artificial intelligence research organization comprising both a non-profit foundation and a controlled for-profit public benefit corporation (PBC), headquartered in San Francisco. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". OpenAI is widely recognized for its development of the GPT family of large language models, the DALL-E series of text-to-image models, and the Sora series of text-to-video models, which have influenced industry research and commercial applications. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization was founded in 2015 in Delaware but evolved a complex corporate structure. As of October 2025, following restructuring approved by California and Delaware regulators, the non-profit OpenAI Foundation holds 26% of the for-profit OpenAI Group PBC, with Microsoft holding 27% and employees/other investors holding 47%.  Under its governance arrangements, the OpenAI Foundation holds the authority to appoint the board of the for-profit OpenAI Group PBC, a mechanism designed to align the entity’s strategic direction with the Foundation’s charter. Microsoft previously invested over $13 billion into OpenAI, and provides Azure cloud computing resources. In October 2025, OpenAI conducted a $6.6 billion share sale that valued the company at $500 billion.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.\\n\\n\\n== Founding ==\\n\\nIn December 2015, OpenAI was founded as a not for profit organization by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), and Infosys. However, the actual capital collected significantly lagged pledges. According to company disclosures, only $130 million had been received by 2019.\\nIn its founding charter, OpenAI stated an intention to collaborate openly with other institutions by making certain patents and research publicly available, but later restricted access to its most capable models, citing competitive and safety concerns. OpenAI was initially run from Brockman\\'s living room. It was later headquartered at the Pioneer Building in the Mission District, San Francisco.\\nAccording to OpenAI\\'s charter, its founding mission is \"to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.\"\\nMusk and Altman stated in 2015 that they were partly motivated by concerns about AI safety and existential risk from artificial general intelligence. OpenAI stated that \"it\\'s hard to fathom how much human-level AI could benefit society\", and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\". The startup also wrote that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible\", and that \"because of AI\\'s surprising history, it\\'s hard to predict when human-level AI might come within reach. When it does, it\\'ll be important to have a leading research institution which can prioritize a good ou'),\n",
       " Document(metadata={'title': 'Scale AI', 'summary': \"Scale AI, Inc. is an American data annotation company based in San Francisco, California. It provides data labeling, model evaluation, and software to develop applications for artificial intelligence.\\nThe company’s research arm, the Safety, Evaluation and Alignment Lab, focuses on evaluating and aligning large language models (LLMs), including through initiatives such as Humanity's Last Exam, a benchmark designed to assess advanced AI systems on alignment, reasoning, and safety. Scale AI outsources data labeling through its subsidiaries, Remotasks, which focuses on computer vision and autonomous vehicles, and Outlier, which focuses on data annotation for LLMs.\\nScale AI's customers in the commercial sector have included Google, Microsoft, Meta, General Motors, OpenAI, and Time. The company also directly works with world governments, including the United States on multiple military-related projects, and with Qatar to improve the efficiency of its social programs.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Scale_AI'}, page_content='Scale AI, Inc. is an American data annotation company based in San Francisco, California. It provides data labeling, model evaluation, and software to develop applications for artificial intelligence.\\nThe company’s research arm, the Safety, Evaluation and Alignment Lab, focuses on evaluating and aligning large language models (LLMs), including through initiatives such as Humanity\\'s Last Exam, a benchmark designed to assess advanced AI systems on alignment, reasoning, and safety. Scale AI outsources data labeling through its subsidiaries, Remotasks, which focuses on computer vision and autonomous vehicles, and Outlier, which focuses on data annotation for LLMs.\\nScale AI\\'s customers in the commercial sector have included Google, Microsoft, Meta, General Motors, OpenAI, and Time. The company also directly works with world governments, including the United States on multiple military-related projects, and with Qatar to improve the efficiency of its social programs.\\n\\n\\n== History ==\\n\\n\\n=== Early years (2016–2019) ===\\nScale was founded in 2016 by Alexandr Wang and Lucy Guo through Y Combinator. The pair previously worked together at Quora. Initial investors of Scale included Dragoneer Investment Group, Tiger Global Management and Index Ventures. Lucy Guo was fired two years later in 2018.\\nIn August 2019, after Peter Thiel’s Founders Fund made a $100 million investment in Scale, its valuation exceeded $1 billion and it acquired unicorn status.\\n\\n\\n=== Growth (2019–2025) ===\\nScale contracted with the United States Department of Defense in 2020.\\nIn May 2021, Michael Kratsios, Chief Technology Officer of the United States under the Trump administration, joined as Scale AI\\'s managing director and head of strategy.\\nBy July 2021, Scale had reached a valuation of $7 billion, after a financing led by Greenoaks, Dragoneer Investment Group and Tiger Global Management. There was an increased demand for data labelling from clients in different industries.\\nIn January 2022, Scale AI won a contract worth $250 million to give American federal agencies access to its suite of tools. In February 2022, Scale AI developed its Automated Damage Identification Service in response to the Russian invasion of Ukraine. Satellite imagery was analyzed, measuring the damage to buildings, which were then geotagged and reported to humanitarian groups. In November 2022, Scale AI was recognized by Time on it’s Best Inventions of 2022 list. The company also opened an office in St. Louis in that same year.\\nIn January 2023, Scale laid off 20% of its workforce.\\nIn May 2023, Scale AI signed a deal with the US Army’s XVIII Airborne Corps, becoming the first AI company to deploy its LLM (known as Donovan) on a classified network.\\nIn August 2023, Scale AI partnered with OpenAI, becoming the company’s \"preferred partner\" to fine-tune GPT-3.5. The company\\'s services were used in the initial creation of ChatGPT. In that same month, Scale AI’s evaluation platform was used at DEF CON, a hacking convention, at its first generative AI red team event, testing models provided by various companies.\\nIn December 2023, Scale AI was among a list of companies that contributed to Meta Platforms’s Purple Llama initiative, a security framework for the purpose of development of open generative AI models.\\nIn February 2024, Scale AI was selected by the Department of Defense to test and evaluate its LLMs for military purposes under a one-year contract.\\nIn March 2024, Scale reached a valuation of almost $13 billion after Accel led another round of funding. In May 2024, Scale raised an additional $1 billion with new investors including Amazon and Meta Platforms. Its valuation reached $14 billion.\\nIn August 2024, Scale signed an agreement with the US AI Safety Institute, collaborating with the agency on research, testing, and evaluation of the company’s AI models. The US AI Safety Institute is controlled by the Department of Commerce’s National Institute of Standards and Technology.\\nIn December 2024, Sca'),\n",
       " Document(metadata={'title': 'History of artificial intelligence', 'summary': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.\\nEventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from James Lighthill and pressure from the U.S. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors\\' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\\nIn the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.\\nInvestment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, causing debate about the future of AI and its impact on society.', 'source': 'https://en.wikipedia.org/wiki/History_of_artificial_intelligence'}, page_content='The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.\\nEventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from James Lighthill and pressure from the U.S. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors\\' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\\nIn the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.\\nInvestment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, causing debate about the future of AI and its impact on society.\\n\\n\\n== Precursors ==\\n\\n\\n=== Myth, folklore, and fiction ===\\n\\nMythology and folklore has depictions of automatons and similar human-like artificial life. \\nIn Greek mythology, Talos was a creature made of bronze who acted as a guardian for the island of Crete. \\nAlchemists in the Islamic Golden Age, such as Jabir ibn Hayyan, attempted Takwin, the artificial creation of life, including human life, although this may have been metaphorical.\\nIn Jewish folklore during the Middle Ages, a Golem was a clay sculpture that was said to have come to life through the insertion of a piece of paper with any of God\\'s names on it into the mouth. 16th century Swiss alchemist Paracelsus described a procedure he claimed would fabricate a homunculus, or artificial man. Brazen heads were a recurring motif in late medieval and early modern folklore.\\nBy the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley\\'s Frankenstein,  Johann Wolfgang von Goethe\\'s, Faust, Part Two, and Karel Čapek\\'s R.U.R. (Rossum\\'s Universal Robots).\\nSpeculative essays, such as Samuel Butler\\'s \"Darwin among the Machines\", and Edgar Allan Poe\\'s \"Maelzel\\'s Chess Player\" reflected society\\'s growing interest in machines with artificial intelligence.\\n\\n\\n==== Automata ====\\n\\nRealistic humanoid automata were built by craftsman from many civ'),\n",
       " Document(metadata={'title': 'Cognition AI', 'summary': 'Cognition AI, Inc. (also known as Cognition Labs), doing business as Cognition, is an artificial intelligence (AI) company headquartered in San Francisco in the U.S. state of California. The company developed Devin AI, an AI software developer.\\nThe company is known for hiring competitive programmers such as Gennady Korotkevich and Andrew He.', 'source': 'https://en.wikipedia.org/wiki/Cognition_AI'}, page_content=\"Cognition AI, Inc. (also known as Cognition Labs), doing business as Cognition, is an artificial intelligence (AI) company headquartered in San Francisco in the U.S. state of California. The company developed Devin AI, an AI software developer.\\nThe company is known for hiring competitive programmers such as Gennady Korotkevich and Andrew He.\\n\\n\\n== Background ==\\nCognition was founded in August 2023 by Scott Wu, Steven Hao, and Walden Yan. All three were competitive programmers who won gold medals at the International Olympiad in Informatics (IOI).\\nOriginally, the company was focused on cryptocurrency, before moving to AI as it became a trend in Silicon Valley following the release of ChatGPT. \\nIn March 2024, Cognition released a demo of its AI coding tool, Devin AI, which was said to be able to perform the tasks of a software engineer.\\nCognition boasted that its 10-person team as of March 2024 has won a total of 10 IOI gold medals. Apart from its founders, other members of the team who are gold medalists include Wu's brother Neal, Gennady Korotkevich and Andrew He. Wu stated this background gives Cognition an edge in AI competition as teaching an AI to be a programmer is a very deep algorithmic problem that requires the system to make complex decisions.\\nThe company was backed by Peter Thiel's Founders Fund which provided $21 million in early 2024, valuing the company at $350 million. In April 2024, Founders Fund led a $175 million investment into Cognition valuing the company at $2 billion making it a Unicorn.\\nIn May 2024, it was announced that Cognition partnered with Microsoft to integrate Devin AI and would be powered by Microsoft Azure. It received a mixed reaction from users online. While Devin AI was considered promising, there were concerns about job displacement and the actual capabilities of Devin AI.\\nIn March 2025, Cognition reached a valuation of $4 billion after a new funding round led by Joe Lonsdale's 8VC.\\nIn April 2025, Cognition released Devin 2.0, which features an integrated development environment designed for AI agent collaboration. It comes with a new plan starting at $20.\\nIn July 2025, Cognition signed a definitive agreement to acquire Windsurf, an agentic IDE, after the fallout from Google's $2.4 billion acquihire of their CEO and other senior employees. After the acquisition, Cognition was valued at $10 billion in September 2025.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website\"),\n",
       " Document(metadata={'title': 'Chai AI', 'summary': \"CHAI AI (also known as CHAI Research) is an American artificial intelligence (AI) company that operates a chatbot platform. Founded in 2021 by William Beauchamp, CHAI's chatbots use large language models (LLMs). The company is headquartered in Palo Alto, California.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Chai_AI'}, page_content=\"CHAI AI (also known as CHAI Research) is an American artificial intelligence (AI) company that operates a chatbot platform. Founded in 2021 by William Beauchamp, CHAI's chatbots use large language models (LLMs). The company is headquartered in Palo Alto, California.\\n\\n\\n== History ==\\nWilliam Beauchamp began developing the initial prototype for CHAI in 2020 while in Cambridge, United Kingdom. The company launched in 2021 and relocated to Palo Alto in 2022.\\nCHAI was one of the first companies to deploy an LLM-based consumer chat application, initially using the open-source GPT-J model.\\nIn July 2024 authorities in Belgium launched an investigation into the company following reports of a Dutch man dying by suicide following extensive chats on the Chai app.\\n\\n\\n== Technology ==\\nCHAI allows users to create and interact with chatbots based on large language models (LLMs). These chatbots can be published to the platform for other users to interact with. \\n\\n\\n== See also ==\\nArtificial human companion\\nList of chatbots\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nChai website\")]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f07b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fb4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2bf26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98d051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23b4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cbbd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1707dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55e873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a38a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e75fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-3-12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
