{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840dcb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_history_aware' from 'langchain_classic.chains' (/home/workstation-p/anaconda3/envs/conda-env-3-12/lib/python3.12/site-packages/langchain_classic/chains/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_classic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_history_aware,create_retrieval_chain\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_classic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine_documents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_chroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'create_history_aware' from 'langchain_classic.chains' (/home/workstation-p/anaconda3/envs/conda-env-3-12/lib/python3.12/site-packages/langchain_classic/chains/__init__.py)"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain_classic.chains import create_history_aware_retriever,create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_ollama import OllamaEmbeddings,ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "import uvicorn\n",
    "\n",
    "embedding=OllamaEmbeddings(model=\"llama3.1:8b\")\n",
    "llm=ChatOllama(model=\"phi4:latest\")\n",
    "\n",
    "\n",
    "st.title(\"Conversational RAG with pdf upload and chat history\")\n",
    "st.write(\"Upload pdf and chat with content\")\n",
    "session_id=st.text_input(\"Session ID\",value=\"default_session\")\n",
    "if 'store' not in st.session_state:\n",
    "    st.session_state.store={}\n",
    "\n",
    "uploaded_files=st.file_uploader(\"Choose a pdf file\",type=\"pdf\",accept_multiple_files=False)\n",
    "\n",
    "if uploaded_files:\n",
    "    documents=[]\n",
    "    for uploaded_file in uploaded_files:\n",
    "        temp_pdf=f\"./temp.pdf\"\n",
    "        with open(temp_pdf,\"wb\") as fl:\n",
    "            fl.write(uploaded_file.getvalue())\n",
    "            file_name=uploaded_file.name\n",
    "        loader=PyPDFLoader(temp_pdf)\n",
    "        docs=loader.load()\n",
    "        documents.extend(docs)\n",
    "    \n",
    "\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=5000,chunk_overlap=200)\n",
    "    splits=text_splitter.split_documents(documents)\n",
    "    vector_store=Chroma.from_documents(documents=splits,embedding=embedding)\n",
    "    retriever=vector_store.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "    contextualize_q_system_prompt=(\n",
    "        \"Given a chat history and the latest user question\"\n",
    "        \"which might reference context in the chat history\"\n",
    "        \"formulate a standalone question which can be understood\"\n",
    "        \"without the chat history. Do NOT answer the question,\"\n",
    "        \"just reformulate it if needed and otherwise return it as is \"\n",
    "    )\n",
    "\n",
    "    contextualize_q_prompt=ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\",\"{input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history_aware_retriever=create_history_aware(llm,retriever,contextualize_q_prompt)\n",
    "    system_prompt=(\n",
    "        \"You are an assistant for question answering tasks.\"\n",
    "        \"use the following pieces of retrieved context to answer\"\n",
    "        \"the question. If you don't know the answer, say that you\"\n",
    "        \"don't know. Use three sentences maximum and keep answer concise\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    qa_prompt=ChatPromptTemplate.from_messages([\n",
    "        (\"system\",system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\")\n",
    "    ])\n",
    "\n",
    "\n",
    "    question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "    rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)\n",
    "\n",
    "\n",
    "    def get_session_history(session:str)->BaseChatMessageHistory:\n",
    "        if session_id not in st.session_state.store:\n",
    "            st.session_state.store[session_id]=ChatMessageHistory()\n",
    "        return st.session_state.store[session_id]\n",
    "\n",
    "    conversational_rag_chain=RunnableWithMessageHistory(\n",
    "        rag_chain,get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\"\n",
    "    )\n",
    "\n",
    "\n",
    "    user_input=st.text_input(\"Ask your question\")\n",
    "    if user_input:\n",
    "        session_history=get_session_history(session_id)\n",
    "        res=conversational_rag_chain.invoke({\"input\":user_input},config={\"configurable\":{\"session_id\":session_id}})\n",
    "        st.write(st.session_state.store)\n",
    "        st.write(\"AI:\",res['answer'])\n",
    "        st.write(\"Chat History:\",session_history.messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4d103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-3-12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
